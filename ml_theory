# Machine Learning
# 
###############################################################################################################
# Machine Learning Concepts:
###############################################################################################################
# Machine learning is the subfield of computer science that gives computers the ability to learn without being "explicitly programmed".
    # Supervised Learning == training model based on features and labels of known dataset to predict unknown labels/ values.
    # Unsupervised Learning == 
    # Machine learning algrithm depends heavily on the extraction of knowledge from the past data,
        # And build a set of calculation based on the patterns found in that particular data.
    # Reinforcement Learning rewards good behaviour and penalize bad bahaviour
#
# Linear, Logictic, Tree Model, NN, LSTM
#
########################################################################################
# Linear Regression 
########################################################################################
    # SST = "Sum of Squares Total" = sum(actual_y - actual_y_mean)^2
    # SSR = "Sum of Squares Regression" = sum(predicted_y - actual_y_mean)^2
    # SSE = "Sum of Squares Error" = RSS = "Residual/Remaining/Unexplained Sum of Squares"
    # SST = SSR + SSE
    # OLS = "Ordinary Least Squares" = "Lowest error"
    # R-squared = SSR/SST = 1-SSE/SST = "Goodness of Fit"
    # 5 tests must be done before doing an OLS Estimation:
        # 1. Linearity (can check visually)
        # 2. No Multicollinearity in independent variables (need to test correlation, can use correlation-matrix)
            # Use (Pearson) Correlations Matrix, to test whether corr < 0.6 or < 0.8
                # Can also use pair plot (relationship between variables)
        # 3. Normality (of the residual/ error term)
            # Can use Q-Q plot to test whether the residuals are normally distributed
                # Normality is assumed for large sample size, based on the Central Limit Theroem
            # Can use Levine's Test
        # 4. Residuals should have Homoscedasticity (Constant Variance)
            # the 'spread' of the residual value does not increase with x (the independent variable)
        # 5. No Autocorrelation between the residuals
            # Linear regression requires that the residuals have very little or no auto-correlation in the data.
                # e.g. error(i+1) is not independent from error(i)
                # Can Use ACF Plot/ Durbin-Watson(DW) Test (Rule of Thumb:  1.5 < d < 2.5 )
        # 6. No Endogeneity(omitted variable resulting in the error term/ error is also related to independent variable)
            # x(i) is not affecting error (e)
                # Can use Hauseman Test
                    Note: Hausman Test is valid only whtn 'Homoscedasticity' criteria is met.
    #
    # Checking Linearity Assumption: DW-Test (target value = 2)
#
########################################################################################
# Logistic Regression
########################################################################################
    # Probablity(x) = e^(a0 + a1*x1 + a2*x2 + a3*x3 + ...) / (1 + e^ (a0 + a1*x1 + a2*x2 + a3*x3 + ...) )
    # or Odds =  Probablity(x)/ (1 - Probablity(x)) = e ^ (a0 + a1*x1 + a2*x2 + a3*x3 + ...)
    # or Log (Odds) = a0 + a1*x1 + a2*x2 + a3*x3 + ... (Preferred Form)
        # Log(p/(1-p)) = a0+ a1*x1 + a2*x2 + --- 
    # Assumption for Logistic Regression is very similar to Linear Regression:
        # 1. Data is free from missing values
        # 2. Target value is either binary or ordinal
        # 3. All predictors are independent from each other
        # 4. There are at least 50 observations in each category (not necessary)

########################################################################################
# Overfitting = the model has focused too much on a particular dataset that the logic does not apply to the other scenario.
    # Increase the training set size
        # (could check with N-Fold cross-validation score)
    # Remove irrelevant variables
        # e.g. using PCA
    # Early Stopping/ Dropouts
        # 
    # Data Augmentation:
        # 
    # Activity Regularrization (need investigation)
        #
    # Reduce Over-fitting in CNN:
        # Random Flipping:
            # 
        # Mirroring: 
        # Skewing: 
#
from sklearn.model_selection import cross_val_score
cross_val_score(DecisionTreeRegressor(), X, y)
#
Under-Fitting: the model has not captured the logic within the data

########################################################################################
# K-Nearest Neighbors (KNN)
    # Based on the K neighbors that are closest to the new data point, find the class that has most existing points in K, and assign new point to that class.
        # Step 1: choose the number of K of neighbors (default = 5)
            # Could use a scatterplot to estimate K
            # Could use elbow method choose K
            # Could use 
        # Step 2: take the K nearest neighbors of the new (target) data point, according to the Euclidean distance
            # Need to scale the variables (normalization)
        # Step 3: Among the K nearest neighbors, count the number of data-points in each catagory
        # Step 4: Assign the new (target) data point to the category/class that you found the most data-points

########################################################################################
# Support Vector Machine
    # SVM is a bit different from other ML algorithm: it uses the "least typical" (rather than most typical) elements in each category, and find a line between the 2
        # When determining the best hyperplane, it has to:
            # 1. Accurately distinguish the different classes
            # 2. Maximize the sum of all distances between each data point and the hyperplane
#

########################################################################################
# Clustering (unsupervised learning)
########################################################################################
    # application of clustering: recommender system, market segmentation, social network analysis, image segmentation, anomaly detection, etc.


########################################################################################
# K-Means Clustering
# K-Means++ 
    # Assign N clusters, 
        # 1. choose the number of clusters, K
        # 2. select random K points, which are temporary centroids
        # 3. assign each data point to the nearest centroid, therefore forms K number of temporary clusters
            # Euclidean Distance: d = 
        # 4. calculate the centroid of each cluster
            # 
        # 5. re-assign the data points based on distence, and recalculate centroid, until the clusters are stable
#
########################################################################################
# Recommender System
########################################################################################
    # 1. User-Based Collaborative (combination of behaviour) Filtering
        # Similarity in user-perference/ user behaviour in the past towards different items.
            # 1. Build a matrix of things that each user had purchases/ viewed
            # 2. Compute the similarity between the users
            # 3. Find users that are similar to you
            # 4. Recommend stuff to you that other people had bought whereas you haven't
                # e.g. people bought this also bought that
            # Problem 1: people's taste changes over time
            # Problem 2: high complexity in calculation, e.g. there are more people than movies
    # 2. Content/Item-based Collaborative Filtering
        # The content-based recommendation is the maximum value of the product of Items and User vectors created from Item-Feature and User-Feature Matrix.
        # Based on the features of the items themselves (e.g. movie type, movie actor, movie year, etc.) amd actions from users.
            # 1. Find each pair of item that were bought/viewed by the same person
            # 2. Measure the similarity of rating/ review from that person who bought/ viewed the 2 items
            # 3. For each item with good feedback/ review, rank the other paired-items by the similarity in (good/ positive) review 
    # 3. Hybrid Filtering
        # Based on existing user preference, recommend items that are similar in features.
    # 4. Popularity Based Filtering
# 2 types of user ratings:
    # Explicit Ratings:
        # e.g. direct review/ rating/ comment, etc.
    # Implicit Ratings:
        # e.g. churn, relative view pattern, etc.
########################################################################################
# Naive Bayes
    # Why "Naive"? = because it is based on the assumption that P(event|X) is independent from each other (which is often untrue)
        # Step 1: P(class-A |X) = P(X | Class-A) * P(Class-A) / P(X)
        # Step 2: P(class-B |X) = P(X | Class-B) * P(Class-B) / P(X)
        # Step 3: Compare P(class-A | X) and P(class-B | X)
    # Multi-class classification: (same logic, calculate P(class-N |X) for each class)

#
########################################################################################
# Principal Component Analysis (PCA) (dimensionality reduction)
    # Find the correlation between variables (variables with high correlation could largely be represented by another variable)
    # PCA is actually tring to learn about the relationship between X and Y, and choose the most influential X variables
        # 1. Standardize the data
        # 2. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.
        # 3. Sort Eigenvalues and choose the most important variables (but still explains much of the variance, e.g. 90%)
    # 


########################################################################################
# Decision Tree
########################################################################################
# Decision tree is actually dividing the observations by "different triangle areas" determined by features, and could be more accurate than simply using linear fitting.
    # Decision Tree Calculation Process:
    # Step 1. Find the root-node + internal-node + leaf with lowest impurity score (e.g. Gini Index)
        # Entropy
        # Gini Impurity:
            # Gini Impurity = 1 - prob_yes ^ 2 - prob_no ^ 2
                # Calculate weighted Gini Impurity 
                # for numeric value, rank the value, get the average in each consecutive pair, and calcultate the weighted average for each scenario, pick the lowest.
    # Step 2. For numeric variables, rank them and calculate the (weighted) impurity score for each mean
    # Step 3. For categorical variables, calculate the impurity score for each combination and choose the node with minimum impurity
# Regression Tree:
    # Getting the average of each class as the predicted result.
    #
    from sklearn.tree import DecisionTreeRegressor
    tree_regressor = DecisionTreeRegressor(random_state = 0, max_depth = 5, max_features = 4) # fix the random seed
    tree_regressor.fit(X, y)
    #
    tree_regressor.predict([[250, 88, 3139, 14.5]])
    #
    
########################################################################################
# Random Forest
    # 1. Base logic is same as Decision Tree, whereas when determining each node, only part of fields/columns/measurements are selected (making it random)
    # 2. Every time a new tree is generated in a Random Forest, it generates a "Complete Tree".
        # Step 1: Pick at random K data points from the training set.
        # Step 2: Build the decision tree associated to thesr K data points.
        # Step 3: Choose the number of trees (N) that you want to build and repeat Step 1 & 2.
        # Step 4: For a new dat point, make each one of the N decision trees to predict the value of Y, and assign the new data point the average value across all trees.
#
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test) # 
rf_error = mse(y_test, rf_pred)
#
# Ada Boost
    # Unlike Random Forest, in Ada Boost model, trees usually contains only 1 node + 2 leaves (called as a stump)
    # Unlike Random Forest where each tree has the same weighting, the "stumps" in Ada Boost are allocated with different weights
        #
    # Unlike Radnom Forest, the order/sequence of stumps matters, as each stump is made by taking the mistake from previous stump into account
#
# Gradient Boost - Regression (simply for numeric results, different from linear regression)
    # Start with a "guess" for a leaf, normally using average of value of y_train
    # Similar to Ada Boost, Gradient boost build "fixed sized trees" based on the previous tree's errors, whereas larger than a stump.
    # Unlike Ada Boost, even though Gradient Boost also scale the trees, it scales all trees with the same %
    # The trees is added until the limit is reached or the prediction is not improving fit % anymore
#
########################################################################################
# XGBoost
#
import pandas as pd
data = pd.read_csv('D:/Collection_Dataset/data/Data.csv')
#
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
#
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)
#
from xgboost import XGBClassifier # XGBRegresser for regression analysis
classifier = XGBClassifier()
classifier.fit(X_train, y_train) # train the classifier
#
from sklearn.metrics import confusion_matrix, accuracy_score
#
y_pred = classifier.predict(X_test)
#
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))
#
from sklearn.model_selection import cross_val_score
#
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
#
print('Average Accuracy for the 10 fold cross_validation = {:.2%}'.format(accuracies.mean()))
print('Standard Deviation of the 10 accuracies = {:.2%}'.format(accuracies.std()))
#
########################################################################################
# ANOVA (Analysis Of Varience) Model 


########################################################################################
# AR(n) Model (auto-regressive) Y(t) = alpha + beta1 * Y(t-1) + beta2 * Y(t-2) + theta 
# VAR Model
    # Y1,t and Y2,t are related (in each time period, our 2+ variables are correlated)


########################################################################################
# Bias-Variance Tradeoff
    # Bias: being unable to find the relationship between dependent and independent variables (unable to fit)
    # Variance: the change of error in different datasets (e.g. low error in training dataset, high error in test or new datasets.)
        # Overfit: if error is high when using test dataset (even though the error is low in train dataset)


########################################################################################
# Monte Carlo Simulation


########################################################################################
# ANN (Artificial Neural Network)
########################################################################################
    # Key benefit of Neural Network is that they could fit "non-linear, non categorical scenario", e.g. a squiggle such as f(x) = 【sin(x) + 1】/2
        # e.g. by adding parameters to a Activition Function( Sigmoid, ReLU, softplus, etc.), the Neural Network could simulate non-linear curves
    # A Neural Network consists Nodes and 

    # Step 1: Randomly assign initial weights and bias (also choose applicable activation function).
    # Step 2: Feed input data (in groups/ mini batches) into the model.
    # Step 3: Pass the result in each layer to calculate result (with initial weights, bias, activation function).
    # Step 4: Prediction (result) is compared to target value and loss is calculated.
    # Step 5: Based on the result/ loss, use backpropagation to re-calculate weights and bias.
    # Step 6: The next epoch (iteration) repeats the process to further adjust the weights and bias (repeat Step 2-5).

# Input Layer
# Hidden Layer
    # Nodes
        # Weights
        # Bias
        # Activication Function:
            # ReLU (Rectified Linear Unit: f(x) = max(0, x) )
    # 
# Output Layer

# Loss Function
# Learning Rate: the size of each adjustment in the iteration process
    # Low Learning Rate = small adjustment in each epoch = more epochs to minimize the loss
    # High Learning Rate = large adjustment in each epoch = less epochs to minimize the loss
    

# Epochs: iterations in the training process

# Back Propagation (for optimizing weights and bias)
    # Chain Rule

# Gradient Descent (applies to linear, logestic, clustering problem, etc.)
#
    #
    # 1. Gradient Descent is most useful when it is very hard/ impossible to solve for the parameters where the derivative = 0
    # 2. When you have 2 or more derivatives of the same function, they are called a Gradient.
        # And we will use this Gradient to descend to the lowest point in the Loss Function, therefore called Gradient Descent.
    # 3. 
    #
    # For Linear Model: 
    # 1. Estimate Function: y = wx + b
    # 2. Loss Function = sum( ( y - y-pred )**2 )
    # 3. Select random x0 and learning rate, then x1 = x0 - derivative_of_Loss_function * Learning_Rate
        # x2 = x1 - derivatite_of_Loss_Function * Learning Rate ... till Loss Function is close to Zero
    # 
# CNN (Convolutional Neural Network) = Feature Extraction + Class Prediction (classification)
    # Step 1: images are fed into convolutional layer, with filters.
        # Convolutional Layer: extract important features in images (by applying filters to images). 
    # Step 2: Feature map is passed to pooling layer, which reduce the size.
        # Pooling Layer: reduce the number of features while retaining the key differentiating features
            # e.g. a 2*2 pooling filter would reduce the matrix size to 25% of its original size
            # Max Pooling:
    # Step 3: A dropping layer randomly drops some features to avoid over-fitting.
    # Step 4: A flattening layer takes the remaining features and convert them into a vector.
    # Step 5: The vector elements are fed into a fully connected network, which generates predictions (classification model).

    # 

#
# Flattening Layer: flattern the feature maps into a vector of values that can be used as input to a fully connected layer.
# Fully Connected Layer: 
#
########################################################################################
# Bagging

########################################################################################
# Boosting

########################################################################################
# Data Normalization
    # Z-Score
        # Z = (X - mean(x))/ stdev(x)
    # Logistic 
        # Z = 1/ (1 + exp(-x))
    # MinMax
        # Z = (X - min(x))/ (max(x) - min(x))
#
########################################################################################
# Gaussian Distribution (normal distribution)


########################################################################################
# Natural Language Processing
    # Text Classification
        # Application: spam email filter, customer request classification, sentiment analysis, new document classification, etc.
    # Automatic Translation
    # Speech Recognition
    # Text generator/ response to conversations
    #
    # Step 1: Load and pre-process text data
    # Step 2: Convert text into feature vectors
    # Step 3: train, score and deploy model
    # 





########################################################################################
# Confusion Matrix:
    #               Predicted(True)     Predicted(False)
    # Actual(True)  True Positive       False Negative
    # Actual(False) False Positive      True Negative
    # 
    # Matrix: [[TP, FN], [FP, TN]]
    # 
    # Recall = TP/ (TP + FN) = True Positive 
    # 
    # Accuracy = (TP + TN)/ (TP + TN + FP + FN) = total number of accurate predictions/ total number of observations
    # Precision = TP/ (TP + FP) = (Among those classified as True, % accurate) 
    # Recall = TP/ (TP + FN) = (For those that are actually True, % identified as True) 
    # 
    # F1 Score = 2/(1/Accuracy + 1/Precision) = 2/ [(2TP + FP + FN)/TP] = 2TP/ (2TP + FP + FN) = 1/ (1 + FP/ 2TP + FN/ 2TP)
        # F1 Score = Weighted average (Harmonic Mean) of Precision and Recall = 2 * Precision * Recall / (Precision + Recall)
# ROC Curve
# AUC = Area Under Curve (e.g. value = 0.8)
    # ROC = Receiver Operating Characteristics
    # ROC Curve = ( FPR on x-axis and TPR on the y-axis )
        # TPR = True Positive Rate = TP / (TP + FN) = Recall
        # FPR = False Positive Rate = FP / (FP + TN) = 



########################################################################################
# Feature Engineering:
########################################################################################
# Feature engineering is the process of using domain knowledge to extract features from raw data, that better represent the data.
    # The features selected could be used to improve the performance of machine learning algorithms.
    # Feature engineering can be considered as applied machine learning itself.
        # A feature is a characteristic that might help when solving the problem.
        # Features are important to predictive models and influence results.
# Feature englineering process:
    # Step 1: brain storming or testing features
    # Step 2: deciding what feature to create
    # Step 3: creating features
    # Step 4: checking how the feature work for your model
    # Step 5: improving your feature if needed
    # Step 6: go back to brain storming/ create more feature until the work is done
# 
# Selecting Features:
    # Adding or dropping features (feature selection):
        # Choose the features that has the most signal/ impact
        # e.g. Permutation Feature Importance
    # Combining multiple features into 1 feature/ Splitting 1 feature into multiple features:
        # represent the data in a more simple way
    # Binning:
        # Convert numeric value into categorical variables.
    # One-hot Encoding:
        # One way to represent categorical data as numbers
    # Cleaning the existing features:
        # impute missing values, remove outliers, scale the data, transform skewed data, etc.
    # Learning New Features:
        # learning word or document embeddings from raw text
 
########################################################################################
# Data Mining:
########################################################################################
 
 
 
 
 
 








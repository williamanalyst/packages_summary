#######################################################################################
# Data Normalization:
    # MinMaxScaler:
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        #
        columns = df.select_dtypes(include = 'numeric').columns
        df[columns] = scaler.fit_transform(df[columns])
    # Z-transformation:
        from sklrarn.preprocessing import StandardScaler
        scaler = StandardScaler()
        #
        #
#######################################################################################
# Label Encoding:
    # 

#######################################################################################
# split train and test dataset
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y) #

    # K-NN (K-nearest neighbors, to predict, choose the closest K points of the item to be predicted, 
        # check to which group do they belong to, and classify it to the group with highest frequency.)
            # large K = smoother decision boundary = less complex model
            # small K = more complex model = risk of over-fitting
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborClassifier(n_neighbors = 6)
    knn.fit(X_train, y_train)
#
    prediction = knn.predict(X_test)
    knn.score(X_train, y_train) # accuracy on the training dataset
    knn.score(X_test, y_test) # accuracy on the test dataset
#
#######################################################################################
# linear regression (ordinary least squares = minimize the sum of residual-squared )
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
#
linearreg = LinearRegression()
linearreg.fit(X_train, y_train) #
#
y_pred = linearreg.predict(X_test)
linearreg.score(X_test, y_test)
linearreg.score(X_test, y_test)
rmse = np.sqrt(mean_squared_error(y_pred, y_test)) # calculate the error/loss function value 

# K-fold cross-validation (resolve the problem that your model selection depends on the train-test split, which may accidentally form unwanted pattern)
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
#
reg = LinearRegression()
cv_score = cross_val_score(reg, X, y, cv = 5) # returns 5 results in 5 folds

# Regularization 
from sklearn.linear_model import Ridge
ridge = Ridge(alpha = 0.1, normalize = True)
ridge.fit(X, y)
#
print(righe.alpha) #

from sklearn.linear_model import Lasso # can be used to select important features of a dataset
lasso = Lasso(alpha = 0.4, normalize = True)
lasso.fit(X, y)
#
lasso_coef = lasso.coef_ # 
#
#######################################################################################
# logistic regression (classification problem, default thresholds = 0.5)
from sklearn.linear_model import LogisticRegression
#
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
#
from sklearn.metrics import roc_curve
y_pred_prob = logreg.predict_proba(X_test)[: 1] # 
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
#
plt.plot(tpr, fpr) # line chart
#
# when threshold = 0, then all prediction = 1


#######################################################################################
# k-means clustering
# 

#
ss = StandardScaler()
df_scaled = ss.fit_transform(df_selected.astype(float)) # standardise the input variables

#
kmeans = KMeans(n_clusters = 3, random_state = 42).fit(df_scaled) # use k-means method to classify the items/observations 
labels = kmeans.labels_

#######################################################################################
# confusion matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
#
y_pred = logreg.predict(X_test)
print(confusion_matrix(y_test, y_pred)) # true label as 1st argument, predicted label as 2nd argument





#######################################################################################
# Decision Tree Model
#
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
#
dtc = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, max_leaf_nodes = 5)
#
dtc.fit(X_train, y_train)
dtc_pred = dtc.predict(X_test)
#
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
#
print('The Accuracy in the training set = {:.2%}'.format(dtc.score(X_train, y_train)))
print('The Accuracy in the test set = {:.2%}'.format(dtc.score(X_test, y_test)))
#
accuracies = cross_val_score(estimator = dtc, X = X_train, y = y_train, cv = 5)
print('Average Accuracy with CV5 = {:.2%}'.format(accuracies.mean()))
print('Standard Deviation with CV5 = {:.2f}'.format(accuracies.std()))











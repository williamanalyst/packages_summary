# DA-100
##############################################################################################
# Exam Structure:
##############################################################################################
# 1. Prepare the Data:
    # Get data from different data sources:
        # Identify and connect to a data source:
            # Flat file location: Local, OneDrive for Business, OneDrive Personal, SharePoint Team Sites.
            # Data from Azure Analysis Services: 
        # Change data source settings:
        # Select a shared dataset or create a local dataset
        # select a storage model:
        # 
            # M Language: q query formula lanaguage that can be used in Power Query Editor in order to prepare the data before it can be loded in the model.
            # DAX Language (Data Analysis Expression): an analytical data calculation language which can be used for in-depth data analysis during the data view phase.

# 2. Model the Data:
    # Design a data model in Power BI:
        # e.g. Star Schema ( 1 fact table + multiple dimension tables )
            # Fact table: contains observational or event data values.
                # e.g. sales order, price, product counts, transactional dates and times, etc.
            # Dimension table: contains the detail about the data in the fact table.
    #
    # Table Properties:
        # General:
            # Field 1: Edit the name and description of the column.
            # Field 2: Add synonyms that can be used to identify the column when you are using the Q&A feature.
            # Field 3: Add a column into a folder to further organize the table structure.
            # Field 4: Hide or Show the Column.
        # FormattingL
            # Field 1: change the data type
            # Field 2: format the date
    #
    # Create a common date table:
        # Can be built in 3 ways:
            # Source Data:
                # specifically identify company holidays, separate calendat and fiscal year, identify weekends versus weekdays, etc.
            # DAX:
                # Dates = CALENDAR(DATE(2011, 5, 31), DATE(2021, 5, 31))
                # Year = YEAR(Dates[Date])
                # Month = MONTH(Dates[Date])
                # WeekNum = WEEKNUM(Dates[Date])
                # DayofWeek = FORMAT(Dates[Date].[Day], "DDDD")
            # Power Query: 
                # New Query --> Blank Query
                # = List.Dates(#date(2021, 05, 31), // start date
                    365 * 10, // mumber of periods
                    #duration(1, 0, 0, 0)) // period = 1 days, 0 hours, 0 minutes, 0 seconds
    #
    # Mark the the official date table.
    #
    # Flatten parent-child hierarchy:
        # Path = PATH(Employee[Employee_ID]. Employee[Manager_ID]) # 
        # Level1 = PATHITEM(Employee[Path], 1)
        # Level2 = PATHITEM(Employee[Path], 2)
    #
    # Cross-filter Direction:
        # Single Cross-filter direction:
            # Only 1 table in a relationship can be used to filter the data.
                # e.g. for a one-to-many or  many-to-one relationship, the cross-filter direction will be from the "one-side", meaning that the filter will
                    # only occur in the table that has the unique values.
        # Both Cross-filter directions or bi-directional cross-filtering:
            # One table in a relationship can be used to filter the other.
                # e.g. a dimension table can be filtered through the fact table, and the fact table can be filtered through the dimension table.
            # You might have lower-performance when using bi-directionsl cross-filtering with many-to-many relationship.
                # You should not enable bi-directional cross-filtering relationships unless you fully understand the ramifications of doing so.
                    # Enabling bi-directional cross-filtering can lead to ambiguity, over-sampling, unexpected results, and potential performance degradation.
    #
    # Creating Measures using DAX:
        # Build Quick Measures:
            # 
        # Create Calculated Columns:
            # New_Columns = 'table1'[column1] * 'table1'[column2]
        # Use DAX to build measures:
        # Discover how content affects DAX measures.
        # Use the CALCULATE function to manipulate filters:
            # The CALCULATE function in DAX is one of the most important functions that a data analyst can learn.
            # The function name "CALCULATE" does not adequately describe what the function is intended to do.
                # The CALCULATE funcion is your method od creating a DAX measure that will override cerrain portions of the context that are being used to the correct result.
                    # e.g. Total_Sales_2015 = CALCULATE(SUM('Sales'[value]), YEAR('Sales'['date']) = 2015)
        # Implement time intelligence by using DAX:
            # 
    # Optimize a model (through making changes to the current state of the data model) for performance: 
        # Review the performance of measures, relationship, and visuals:
            # 90% of poor performance come from a bad data model, or bad Data Analysis Expression (DAX)
            # Option 1: Ensuring that the correct data types are used.
            # Option 2: Deleting unnecessary columns and rows.
            # Option 3: Avoiding repeated values.
            # Option 4: Replacing numeric columns with measures.
            # Option 5: Reducing Cardinalities.
            # Option 6: Analyzing model metadata.
            # Option 7: Summarizing data where possible.
                # Analyze Performance: (performance analyzer)
                    # Performance analyzer will help you identify elements that are contributing to your performance issues, which can be used during troubleshooting.
                    # Before you run performance analyzer, to ensure you get the most accuract results in your analysis/ test, 
                        # you should start with a clear visual cache and clear data engline cache:
                            # Visual Cache:
                                # When you load a visual, you can't clear this visual cache without closing the Power BI Desktop and opening it again.
                                    # To aviod any caching in play, you need to start your analysis with a clean visual cache.
                            # Data Engline Cache:
                                # When a query is run, the resutls are cached, so the results of your analysis will be misleading.
                                # You need to clear the data cache before running the visual.
                                    # To clear the cache, you can either restart Power BI Desktop or connect DAX Studio to the data model and then call Clear Cache.
        # Resolve issues and optimize performance:
            # Visuals: 
                # Option 1: reduce the number of visuals on the report page.
                # Option 2: Reduce the number of fields in each visual.
            # DAX Query:
                # less than 120 milliseconds would be a good indicator.
            # Correct Relationship:
            # Matadata:
                # Metadata is information about other data.
                # Power BI metadata contains information on your data model, such as the name, data type and format of each column, schema of the database,
                    # the report design, when the file was last modified, the data refresh rates, etc.
                # When you load data into Power BI desktop, it is good practice to analyze the corresponding metadata so you can identify any inconsistencies.
                    # Thtough analyzing the metadata, you could identify unnecessary columns, errors within your data, incorrect data types, volume of data loaded, etc.
                # You could use the Power Query editor in Power BI Desktop to examine the columns, rows, and values of the raw data.
                    # The Power Query Options include:
                        # Unnecessary Columns:
                        # Unnecessary Rows:
                        # Data Type:
                        # Query Names:
                        # Column Details:
                            # Column quality:
                            # Column distribution:
                                # Distinct Values Count: the total number of different values found in a given column.
                                # Unieue Values Count: the total number of values that only appear once in a given column.
                            # Column profile:
                # Auto Date/time feature:
                    # Power BI Desktop automatically creates a hidden calculated table for each data column, provided that cettain conditions are met.
                        # The auto date/time option allows you to work with time intelligence when filtering, grouping, and drilling down through calendar time periods.
                            # Users are recommended to keep auto date/time option enabled Only when working with calendar time periods and when having a simple model.
                            # Turn off the auto date/time option: file --> options and settings --> options --> Global/ Current file --> data load --> time intelligence
        # Use variables to improve performance and troubleshooting:
            # Improved performance:
                # Variables can make measures more efficient because they remove the need for Power BI to evaluate the same expression multiple times.
            # Improved readability:
                # variables have short, self-explaining names and are used in place of an ambiguous, multi-worded expression.
                    # Users might find it easier to read and understand the formulas when variables are used.
            # Simplified debugging:
                # You could use variables ti debug a formula and test expressions, which can help during trouble-shooting.
            # Reduced complexity:
                # 
            # e.g. Sales_YOY_Growth = 
                    VAR SalesPriorYear = CALCULATE([Sales], PARALLELPERIOD( 'Date'[Date], -12, MONTH ) )
                    VAR SalesVariance = DIVIDE ( ([Sales] - SalesPriorYear), SalesPriorYear)
                    RETURN SalesVariance
        # Improve preformance by reducing cardinality levels:
            # Cardinality is the term that is used to describe the uniqueness of the value in a column.
            # Cardinality is also used in the context of the relationships between 2 tables, where it describes the direction of the relationship.
                # When distinct count is high --> low level of cardinality --> better performance
                # When discount count is low --> high level of cardinality --> poorer performance
            # e.g. Use a summary table from the data source:
                # Where a detail table mighr contain every transaction, a summary table would contain 1 record per day/ week/ month, with average, etc.
        # Optimize DirectQuery with table level storage:
            # The performance of the model will not only be impacted by the performance of the underlying data source, but also:
                # influenced by the network latency, the performance of the data source's server and how many other workloads are running, etc.
            # Benefits of DirectQuery:
                # Condition 1: It is suitable in case where the data changes frequently and near real-time reporting is required.
                # Condition 2: It can handle large data without need to pre-aggregation.
                # Condition 3: It applies data sovereignty restrictions to comply with legal requirements.
                # Condition 4: It can be used with a multidomentionsl data source that contains measures such as SAP Business Warehouse (BW).
            # If your organizations needs to use DirectQuery, you should clearly understand its behaviour within Power BI Desktop and be aware of its limitations.
                # You will then be in a good position to take action to optimize the directquery model as much as possible.
            # Limitations of DirectQuery connections:
                # Limit 1: Performance, which is dependent on the data source.
                # Limit 2: Security, to aviod situations that certain users should not have access to some data involved.
                # Limit 3: Data Transformation (certain data source does not allow data transformation)
                    # e.g. when you connect to OLAP source, you cannot make any transformation at all, the entire external model is taken from the data source.
                # Limit 4: modelling
                    # some modelling capabilities only exist for imported dataset, whereas not available or limited to DirectQuery
                # Limit 5: reporting
                    # Quick insighrs and Q&A features are not supported for DirectQuery mode.
                    # Also, the use of the explore feature in Excel will likely to result in poorer performance.
            # Optimize the underlying data source (connected database):
                # Aviod the use of complex calculated columns because the calculalation expression will be embedded into the source queries.
                    # It is more efficient to push the expression back to the source because it aviods the push down.
                    # You could also consider adding surrogate key columns to dimension-type tables.
                # Review the indexed and verify that the current indexing is correct.
                    # If you need to create new indexes, ensure that they are appropriate.
            # Customize the Query reduction options:
                # Option 1: Reduce nnumber of queries sent by
                    # By default, every visual interacts with every other visual.
                        # You can optionally choose which visual interact with each other by using the "Edit Interactions" featu
                # Option 2: Slicers
                    # By default, the instantly apply slicer changes option is selected.
                        # To force the report users to manually spply slicer changes, select the "Add an Button to each slicer to apply changes when you are ready" option.
                # Option 3: Filters
                    # By default, the instant apply basic filter changes option is selected.
                        # Add an apply button to all basic filters to apply changes when you are ready.
                        # Add a single apply button to the filter pane to aooly changes at once.
        # Create and manage aggregations:
# 3. Visualize the Data:
    # 
# 4. Analyze the Data:
    # 
# 5. Deploy and Maintain Deliverables:
    # 
#
# Power BI Workflow:
    # Get Data (M Formula) --> Add Modelling (DAX Formula) --> Visualize Data --> Share Data

##############################################################################################
# Core Component of Data Analytics:
##############################################################################################
# 1. Descriptive Analytics:
    # Descriptive analytics help answer questions about what happened based on historical data, summarize large datasets to describe outcomes to stakeholders.
        # By developing KPIs, the metrics help track the success or failure of key objectives.
            # e.g. generating reports to provide an overview of an organization's sales or financial data.
# 2. Diagnostic Analytics:
    # Step 1: Identify anomalies in the data, e.g. unexpected changes in a metric or particular market segment
    # Step 2: Collect data that is relevant to these anomalies
    # Step 3: Use statistical techniques to discover relationships and trends that explain the anomalies.
    #
    # Diagnostic analytics help answer questions about why events happened.
    # Diagnostic analytics techniques suppliment basic descriptive analytics, and they use the findings from descriptive analytics to discover the cause of the events.
        # Then KPIs are further investigated to discover why these events improved or become worse.
# 3. Predictive Analytics:
    # Predictive analytics help answer questions about what will happen in the future.
    # Predictive analytics techniques use historical data to identify trends and determine if they are likely to recur.
        # e.g. neural network, decision tree, regression
# 4. Prescriptive Analytics:
    # Prescriptive analytics help answer question about which actions should be taken to achieve a certain goal or target.
    # This technique allows businesses to make informed decisions in the face of uncertainty.
        # Prescriptive analytics technique rely on machine learning strategies to find patterns in large datasets.
        # By analyzing past decisions and events, organizations can estimate the likelyhood of different outcomes.
# 5. Cognitive Analytics:
    # Cognitive analytics attempt to draw inferences from existing data and patterns, derive conclusions based on existing knowledge bases, and add these findings
        back to the knowledge base for future inferences, a self-learning feedback loop.
    # Cognitive analytics help you learn what might happen if circumstances change and determine how you might handle these situations.
        # Inferences are not structured queries based on a rules database, rather, 
            # they are unstructured hypotheses that are gathered from several sources and expressed with varying degrees of confidence.
        # Effective cognitive analytics depend on machine learning algorithms, and will use several natural language processing concepts to make sense of
            # previously untapped data sources, e.g. call center conversations, product review comments, etc.
##############################################################################################
# Roles in data analytics:
##############################################################################################
# 1. Business Analyst: 
    # Closer to the business, with higher domain knowledge, and better interpretablity towards data in that domain.
# 2. Data Analyst:
    # Area 1: Prepare
    # Area 2: Model
    # Area 3: Visualize
    # Area 4: Analyze
    # Area 5: Manage
    #
    # Responsible for profiling, cleaning, and transforming data.
        # e.g. designing and building scalable and effective data models.
        # e.g. enabling and implementing the advanced analytics capabilities into reports for analysis.
        # e.g. management of Power BI assets, including reports, dashboards, workspaces and underlying datasets that are used in the reports.
        # e.g. implementing and configuring proper security procedure, in conjunction with stakeholder requirements, to ensure safekeeping of all Power BI assets and data.
        # e.g. data analysts work with data engineers to determine and locate appropriate data sources that meet stakeholder requirements.
# 3. Data Engineer: (ingesting, transforming, validating and cleaning data to meet business needs and requirements)
    # Data engineers provision and set up data platform techniques that are on-premises and in the cloud.
        # Manage and secure the flow of structured and unstructured data from multiple sources.
            # The data platform that data engineers use include relational database, non-relational database, data stream, and file stores.
            # Data engineers also ensure that the data services securely and seamlessly integrate across data services.
# 4. Data Scientist: 
    # Data scientists perform advances analytics to extract value from data.
        # e.g. descriptive analytics that evaluate data through EDA
        # e.g. predictive analytics such as machine learning models to detect anomalies or patterns.
# 5. Database Administrator: (monitors and manages the overall health of a database and the hardware that the database relies on.)
    # Database administrators implements and manages the operational aspects of cloud-native and hybrid data platform solutions that are built on 
        Microsoft Azure data service and Microsoft SQL Server.
    # A database administrator is responsible for the overall availability and consistent performance and optimization of the database solution.
        # Database administors work with stakeholders to identify and implement the policies, tools and processes for data backip and recovery plans.
        # Database administors are also responsible for managing the overall security of the data, granting and restricting user access and previleges to the data, etc.
##############################################################################################
# Power BI: (a collection of services, apps and connectors that lets you connect to your data)
##############################################################################################
# Basic Building Block 1: Visualizations
    # a visual representation of data, also called visuals
# Basic Building Block 2: Datasets
    # A dataset is a collection of data that Power BI uses to create its visualizations.
        # Datasets can also be a combination of many different sources, which you can filter and combine to provide a unique collection of dataset for Power BI usage.
        # e.g. combining 3 databases, 1 website table, 1 excel table could still be considered as 1 single dataset.
# Basic Building Block 3: Reports
    # A report is a collection of visualizations that appear together on 1 or more pages.
# Basic Building Block 4: Dashboards
    # For the sharing of either a report or a collection of visualizations. (must fit on a single page, often called a canvas)
# Basic Building Block 5: Tiles
    # A tile is a single visualization on a dashboard. 




# Segment 1: Power BI Desktop (Microsoft Windows desktop application)
# Segment 2: Power BI Servier (online SaaS service)
# Segment 3: Mobile Power BI Apps (available on any device, with native mobile BI apps for Windows, iOS, and Andriod)
#






##############################################################################################
# Data Concepts:
##############################################################################################
# Data repository:
# Data Processing:
# Data Dimension:
    # Must be uniquely separated dimensions
# Incremental Refresh:
    # 
# Query Folding:
    # 
# Data Granularity:
# Directionality/ Cardinality to a relationship:
    # 

##############################################################################################
# Questions:
##############################################################################################
# When you create a workspace:
    # you need to add the name of the office 365 group.
#
# Security requirements for management of reports and dashboards:
    # Create a workspace for development
    # Create a depolyment pipeline and assign the development workspace to the development stage.
    # Define data source rules, and deploy to test, and production.
#
# How to control access to data in a certain department?
    # Option 1: Publish the app from the department's workspace and add the users in the department to the app.
    # Option 2: Assign the viewer role in the department to the users in the department.
# 
# How should you provide access for board members (external users)?
    # Add the Board members as guests in Azure Active Directory B2B, create an app, and invite them as guest users.
        # Should not add user as Office 365 workspace, which also grant access in building/ modifying reports.
        # Meanwhile, workpace is not user-friendly for end-users without prior knowledge.
        # Should not share app with email address, because people with external email addresses cannot see Power BI contents unless they are Azure AD B2B guest users.
# 
# You use Common Data Service as a data source, and use Power Query to connect and get the data.
    # Why is the "View Native Query" option disables whereas you want to investigate query performance issues.
        # The Common Data Service does not support Query folding.
#
# How to distribute a report among executives without giving them direct access to the workspace?
    # Step 1: Select the department workspace
    # Step 2: Click the "Publish App" button.
    # Step 3: In the "Setup" tab, provide the app name and description.
    # Step 4: In the Permission tab, specify the users who can view the app.

# Data Classification:
    # Data classification apply to the entire organization and cannot be limited to a specific group.
        # You cannot restrict a dashboard based on membership of an Azure AD security group.
    # Data classification for a dashboard is only displayed when the administrator has checked the "Show Tag" option for the classification.
        # 
#
# 
































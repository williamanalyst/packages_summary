# Data Modelling:

################################################################################################
# Machine Learning
################################################################################################
# Machine Learning --> use algorithms to find patterns and generate models without human intenvision

################################################################################################
# Supervised Machine Learning
################################################################################################
# Supervised Machine Learning --> target variables and input variables are known --> algorithm generates a model that establish relationship between input and target
    # e.g. decision tree
    
#
################################################################################################
# Unsupervised Machine Learning
################################################################################################
# Unsupervised Machine Learning --> identify 1 or mre homogeneous segments/ subsets of the data based on:
    # 1. the degree of similarity among observations/ inputs
    # 2. how many subsets there should be
    # 3. how common or rare the subsets are --> 

################################################################################################
# Clustering Analysis: (e.g. for customer segmentation)
################################################################################################
# There are 2 broad types of clustering: hard clustering & soft clustering
    # Hard Clustering: each point must be assigned to 1 of the clusters
    # Soft Clustering: instead of putting each point into a separate cluster, a probablity of that data to be assigined to a cluster is assigned

# Types of Clustering Algorithms:

    # Centroid Clustering:
        # Choose the number of clusters we are expecting. (e.g. K-Means)
            # e.g. cat/ dog --> 2 centroids
            # calculate how far away each observation is from the centroids selected.
            # recalculate the centroids until the classification is stable
    # Density Clustering: (e.g. DBSCAN, OPTICS)
        # Group observations based on how closely (high density) they are related to each other.
            # Step 1: select a random point
            # Step 2: get a number of observations into a group if the 
    # Distribution Clustering: (often suffers from overfitting)
        # Possibility that each observation would belong to the different clusters.
    # Connectivity Clustering: (e.g. hierarchical clustering)
        # Based on the notion that data points closer in data space exhibit more similirity to each other.
        # Method 1: start with clustering all data points into separate clusters and then aggregate them with distance decreases
        # Method 2: each data point is treated as a group, and partitioned as the distance/ threshold increases

################################################################################################
# Hierarchical Clustering: (often visualized using heatmap)
################################################################################################
# Step 1: Find the distance between each data point and its nearest neighbor (and rank them)
# Step 2: Linking the most nearby neighbors (and form smaller subgroups)
    # The number of subgroups could be selected by using the demdrogram
        # Dendrogram is a tree-graph that is commonly used for visualizing taxonomies, lineage and relatedness.

# 


################################################################################################
# Heatmap:
################################################################################################



################################################################################################
# Reinforcement Machine Learning
################################################################################################
# Reinforcement Machine Learning --> reward for correct & punish for error --> improving with more and more tests/ trials
    # 






################################################################################################
# Combine Cluster Analysis and Decision Trees together:
################################################################################################
# 1. 
# 2. 
# 3. 
# 4. categorical data/ variables are usually easier to obtain --> e.g. gender, zip code, education, number of dependents, etc.

# 


################################################################################################
# BIRCH/ 2-step model:
################################################################################################
# e.g. recommend the best number of clusters (default is BIC criterion)




################################################################################################
# Self-organizing Map:
################################################################################################
# 


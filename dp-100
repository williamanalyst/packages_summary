# DP100 Exam Curriculum
    # 1. Set-up Azure ML Workspace (create ML workspce, manage data objects)
        # Datastore: 
        # Dataset: 
    # 2. Run Experiment and Train Models (create model with Azure ML Designer, run script in ML Workspace, generate metrics, automate training process)
    # 3. Optimize and Manage models (use automated ML to optimize model, use Hyperdrice to tune parameters, use model explainer to interpret model, manage model)
    # 4. Deploy and Consume Models (create compute production target, deploy model as a service, create pipeline, publish pipeline as a web service)
# 
######################################################################################################
# Azure ML Workspace (Architecture/ Structure)
    # Managed Resources (compute instance/ cluster)
    # Linked Service (data storage, compute target)
    # Assets (environments, experiments, pipelines, datasets, models, endpoints)
    # Dependencies (Azure Storage Account, Azure container registry, Azure Key Vault, Azure Application Insights)
# 
# 
######################################################################################################
# AI (Artificial Intelligence) = computer performing tasks without being specifically programmed to.
 
# 
# Azure ML service stores metrics and metadata in Azure Cosmo DB instance where all data is encrypted at rest. 
# 
 
# 
 
# Azure Container Instance

# Endpoint

# Inference Cluster
 
# Resource Group
 
# Azure ML Experiment = named process, usually the running of a script or pipeline, that can generates metrics/ outputs.
    # and can be tracked in the Azure ML workspace
# 
# Azure Cognitive Service
# Azure Data Lake Analytics
# Azue HDInsight with Spark Mlib
# 
######################################################################################################
# Outliers: observations that have much higher distance compared to other observations
    # outliers does impact the prediction/ estimation of the sample/ model
    # outliers could result from human input error, malfunction of measurement equipment, data transmission error, system bebaviour eror,
        # feaudulent behaviour, natural outliers, sampling error, etc.
    # boxplot, histogram and scatterplot are recommended to identify outliers visually
        # as well as percentile analysis (most common method is visualization)
    #
    
# Variables at different scale
    # Normalize the dataset
# Too many variables or curse of dimensionality
    # PCA
# Missing Value in a large dataset
    # replace with MICE
# Unbalanced Dataset (e.g. fraud data)
    # SMOTE
# Merge datasets on column values
    # join data


 
 
 
 
# 
######################################################################################################
# Azure Identity Servive
    # Authentication (credentials, such as username + password) = prove who you are
    # Authorization (grant access based on permission) = what you are allowed to do based on who you are
# 
# Azure Active Directory (ID as a service/ IDaaS)
    # when the service involve both cloud and on-premise, or through external internet, there is not a central location to authenticate users.
        # 
    
# 
    # Single Sign on (SSO) = same access for multiple apps
    # Role-based access 
# 
# Azure Resource Lock (Cannot delete or read only) = 
    # Locks are interited from the parent group/ resource
# 
 
 
######################################################################################################
# Azure Policy 
 
 
# Azure Blueprint 

######################################################################################################
 
 

 
 
 
# Mean Deviation (when the number of observations is small)
# Standard Deviation (when the number of observations is large)
    # describe how the data is distributed around the mean
    # std(x) = [ sum_of_squared_distances/ (n-1) ] ^ 0.5
        # when the mean of 2 datasets are similar, std(x) could be comparable
# Quartile
    # 1st quartile, median, 3nd quartile
    # IQR = Inter Quartile Range = Q3 - Q1
# Skewness
    # Skewness the a measure of the asymmetry of the probablity distribution of a real-valued random variable about its mean.
        # Positive Skewness = Skewed to left, long tail to right = mode < median < mean
        # Negative Skewness = Skewed to right, long tail to left = mean < median < mode
# Kurtosis
    # 
 
######################################################################################################
# Supervised Learning: 
    # Classification
        # Logistic Regression
        #
        # Decision Tree: (could be for both categorical and continuous variables)
            # Boosted Decision Tree:
            # Decision Forest:
            # Decision Jungle:
        #
    # Regression
        #
# Unsupervised Learning: does not use previously known label values to train the model 
    # K-means Clustering: nominate the number of clusters (K) 
        # Centroid: center point of each cluster (each point is assigned to the nearest centroid) 
            # Each centriod is moved to the center of the data points based on the mean distance of all points within that cluster
            # After moving the centroid, some data points may be closer to a differetn centriod (therefore the cluster will be re-assigned for that data point)
            # The centroid movements stop until the clustering is stable or the maximum steps allowed is reached 
    # SVM 
        # Accuracy is more important than the margins in SVM
# 
# Reinforcement Learning: 
 

    # 


# Feature Selection
    # for simplification of models so that they could be interpreted and understood more easily
    # for shorter training time
    # for better accuracy
    # for avoiding the curse of dimensionality
    # for enhanced generalization that helps reducing over-fitting
        # Filter-based Method: use the correlation of x1, x2, x3, ... xn to Y, and choose the most relevant/ influential x
            # e.g. Pearson Correlation, Chi-squared, Mutual Information, Fisher Score, etc.
        # Wrapper Method: test with different combinations of x1, x2, x3, ... xn, and choose the combination of variables that has the best performance
            # e.g. accuracy/ precision or RMSE based on different models
            # Forward Selection: add 1 variable after 1 variable
            # Backward Elimination: eliminate 1 variable after 1 variable
            # Recursive Feature Elimination: random select variables in each iteration, keep the most important variables and removes least influential ones.
        # Embedded Method: 

# Impact of Split Percentage:
# Impact of Stratification:
# Low L1 and High L2:
# High L1 and Low L2:
# High L1 and High L2:
# Low L1 and Low L2: 


######################################################################################################
# Azure ML Studio - Modules
#
# Summarize Data: 
    # gemeral summary of a dataframe (min, max, mean, mode, missing data, etc.)
# Clip Values: 
    # set of threshold
        # 
# MICE: multi-variate imputation using chained equation or multiple imputation by chained equations
    # assuming that the data is missing randomly (not systematically or biased to a reason)
    # Step 1: calculate the mean value based on available data
    # Step 2: replace all missing value with mean
    # Step 3: choose dependent column and restore original (for the 1 column that you would like to fill/ solve)
    # Step 4: apply transformation and create prediction model
    # Step 5: predict missing value and repeat step 3-5 (for other columns with missing value)


# Filter Based Feature Selection
    # corr(x1, y)
# Fisher Linear Discriminant Analysis
# Permutation Feature Importance


# Hyperparameters Tuning: (every module has a set of input parameters, called hyperparameters)
    # 
    # Optimization Tolerance: when a new iteration does not decrease the loss function (e.g. delta loss < 10^-5 )
    # L1 Regularization: (Lasso Regression)
        # Lasso regression can shrink some coefficients to zero, which means performing variable selection
    # L2 Regularization: (Ridge Regression)
        # Ridge regression will shrink all the coefficientby the same scale, which will not eliminate any variable


# Evaluate Model Module:
    # Confusion Matrix:
        #               Predicted(True)     Predicted(False)
        # Actual(True)  True Positive       False Negative
        # Actual(False) False Positive      True Negative
        # 
        # Matrix: [[TP, FN], [FP, TN]]
        # 
        # Recall = TP/ (TP + FN) = True Positive 
        # 
        # Accuracy = (TP + TN)/ (TP + TN + FP + FN) = total number of accurate predictions/ total number of observations
        # Precision = TP/ (TP + FP) = (Among those classified as True, % accurate) 
        # Recall = TP/ (TP + FN) = (For those that are actually True, % identified as True) 
        # 
        # F1 Score = 2/(1/Accuracy + 1/Precision) = 2/ [(2TP + FP + FN)/TP] = 2TP/ (2TP + FP + FN) = 1/ (1 + FP/ 2TP + FN/ 2TP)
            # F1 Score = Weighted average (Harmonic Mean) of Precision and Recall = 2 * Precision * Recall / (Precision + Recall)
    # AUC = Area Under Curve (e.g. value = 0.8)
        # ROC = Receiver Operating Characteristics
        # ROC Curve = ( FPR on x-axis and TPR on the y-axis )
            # TPR = True Positive Rate = TP / (TP + FN) = Recall
            # FPR = False Positive Rate = FP / (FP + TN) = 
######################################################################################################
# Question: Introduce Docker for Windows to attendee
    # 
# Question: Select Data Science Environment
    # 
 
# Question: Move large dataset from Azure ML Studio to Weka environment
    # 
    
#   
 
# DSVM (Data Science Virtual Machine)
 
# 




 
 

# DP100 Exam Curriculum
######################################################################################################
# 1. Set-up Azure ML Workspace (create ML workspce, manage data objects)
######################################################################################################
    # Create Workspace: (4 Dependencies: Azure Storage Account, Azure Container Registry, Azure Key Vault, Azure Application Insight)
        # Linked to: Datastores, Compute Targets
        # Assets include: Environment, Experiment, Pipelines, Datasets, Models, Endpoints
        # Managed Resources: Compute Instances, Compute Clusters
        from azureml.core import Workspace
        Workspace.create(name = workspaceName1, subscription_id = subscription_id, resource_group = resource_group, 
                                                location = 'australiaeast', # minimum 4 parameters above
                                                private_endpoint_config = private_endpoint_config, sku = 'enterprise')
        # Note: The Azure ML designer is only availabel with the enterprise edition of the workspace.
    # Datastore: Datastore securely connect to your storage service on Azure without putting in your authentication credentials, 
        # and the integrity of your original data source at risk. They store connection information, e.g., your subscription ID and token authorization
        # in you Azure Key Vault that is associated with this workspace, so you can securely access your storage without having to hard code them in your script.
            # You can create datastores that connect to these Azure store solutions.
        # Azure ML designer will automatically create a datastore named azureml_globaldatasets when you open a sample in the designer homepage.
            # This datastore only contains sample datasets. User should not use this datastore for any confidential data access.
        from azureml.core import Datastore
        ws = Workspace.from_config(path = './config1.json')
        az_datastore = Datastore.register_azure_blob_container(workspace = ws, datastore_name = 'azure_sdk_blob01', 
                                            account_name = '', container_name = '', account_key = '')
    # Dataset (there are 2 dataset types, based on how users consume them in training; FileDataset and TabularDataset): 
        # FileDataset:
                # If your data is already cleansed, and ready to use in training experiments, 
        # Tabular Dataset:
            # 
######################################################################################################
# 2. Run Experiment and Train Models (create model with Azure ML Designer, run script in ML Workspace, generate metrics, automate training process) 
######################################################################################################
    # Note: when using the Azure ML designer, the compute = Azure ML Compute Cluster, when using Azure ML SDK, compute target = local computer. 
    # 
    # 
    #
    # Run Automated ML Experiment:
        from azureml.core.experiment import Experiment
        from azureml.widgets import RunDetails
        #
        print('Submitting Auto ML Experiment')
        automl_experiment = Experiment(ws, 'experiment_name')
        automl_run = automl_experiment.submit(automl_config)
        RunDetails(automl_run).show()
        automl_run.wait_fot_completion(show_output = True)  # 
        #
        best_run, fitted_model = automl_run.get_output()
        print(best_run)
        #
######################################################################################################
# 3. Optimize and Manage models (use automated ML to optimize model, use Hyperdrive to tune parameters, use model explainer to interpret model, manage model)
######################################################################################################
    # Use Azure Hyperdrive to turn Hyperparameters (also known as hyperparameter optimization)
        # Performance Metric to Measure: Accuracy, Precision, Recall, AUC, F1 Score
            # Step 1: Select a Sampling Method (Random sampling, Grid sampling, Bayesian Sampling)
            # Step 2: Define the parameter Search Space
            # Step 3: Define Primary Metric
            # Step 4: Define Early Termination Options: (Bayesian sampling does not allow early termination)
                # Option 1: Bandit Policy
                    from azureml.train.hyperdrive import BanditPolicy
                    early_termination_policy = BanditPolicy(slack_amount = 0.1, evaluation_interval = 1, delay_evaluation = 10)
                        # early termiation start from 10 dalays, and check each iteration from 11th epoch, and if new accuracy < max(past performance) - 0.1 then stop.
                # Option 2: Median Stopping Policy
                    from azureml.train.hyperdrive import MedianStoppingPolicy
                    early_termination_policy = MedianStoppingPolicy(evaluation_internal = 1, delay_evaluation = 5)
                        # early termination start from 5 delays, and check each epoch run, stop if new accuracy < median(past performance)
                # Option 3: Truncation Selection Policy (do not keep the iterations that has worst performance)
                # Early termination only applies to SGBClassifier, SGDRegressor, XGBoost, CNN/RNN (deep learning algorithm), etc.
            # Step 5: Find the model with optimal hyperparameter values (Bayesian Sampling uses Bayesian optimization for getting the best combination.)
                # Note: 
        # Use Model Explainer to Interpret Models:
            # Select Model Interpreter: (without specifying the features and classes, the explainer could still work, just without labels.)
                # Mimic Explainer: 
                    # An explainer that creates a surrogate(replacement) model that approximates your trained model and can be used to generate explainations.
                        # This explainable model must have the same kind of architecture as your trained model (e.g. linear, or tree-based). 
                # Tabular Explainer:
                    # An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate
                        # for your model architecture.
                    # SHAP Tree Explainer:
                    # SHAP Deep Explainer:
                    # SHAP Linear Explainer:
                    # SHAP Kernel Explainer: 
                # Permutation Feature Importance (PFI) Explainer:
                    # Analyzes the feature importance by shuffling feature values(e.g. remove 1 variable randomly) and measuring the impact on perdiction performance.
                # Note: azureml.interpret package can be installed locally to create explainers.
                #
                from interpret.ext.blackbox import MimicExplainer
                from interpret.ext.glassbox import DecisionTreeExplainableModel
                mimic_explainer = MimicExplainer(model = loan_model, initilization_examples = X_test, 
                                                 explainable_model = DecisionTreeExplainableModel, 
                                                 features = ['loan_amount', 'income', 'age', 'marital_status'],
                                                 classes = ['reject', 'approve'])
                # a tabular dataset will be processed, an explainer will be loaded based on a model type.
                from interpret.ext.blackbox import TabularExplainer
                tab_explainer = TabularExplainer(model = loan_model, initilization_examples = X_test, 
                                                 features = ['loan_amount', 'income', 'age', 'marital_status'], 
                                                 classes = ['reject', 'approve'])
                # 
                from interpret.ext.blackbox import FPIExplainer
                pfi_explainer = PFIExplainer(model = loan_model, features = ['loan_amount', 'income ', 'age', 'marital_status'],
                                                                 classes = ['reject', 'approve'])
            # Generate feature importance data:
                # Shapley Value: 
                # Local and Global Interpretability:
            # Global Feature Importance:
                # Global Feature Importance: based on either training data or test dataset (result from Global and Local Feature Importance may be different)
                # Global feature importance quantifies the relative importance of each feature in the test dataset as a whole.
                # 
                global_mimic_explanation = mimic_explainer.explain_global(X_train)
                global_mimic_feature_importance = global_mimic_explanation.get_feature_importance_dict()
                #
                global_tab_explainer = tab_explainer.explain_global(X_train)
                global_tab_feature_importance = global_tab_explainer.get_feature_importance_dict()
                # 
                # Unlike mimic/ tabular explainer, PFI Explainer requires the actual label:
                global_pfi_explanation = pfi_explainer.explain_global(X_train, y_train)
                global_pfi_feature_importance = global_pfi_explainer.get_feature_importance_dict()
            # Local Feature Importance:
                # Local Feature Importance: based on certain observations only
                # Local feature importance measures the influence of each feature value for a specific individual prediction.
                # Unlike mimic/ tabular explainer, PFI Explainer does not support local feature importance explanations.
                #
                local_mimic_explanation = mimic_explainer.explain_local(X_test[0:5])
                local_mimic_features = local_mimic_explanation.get_ranked_local_names()
                local_mimic_importance = local_mimic_explanation.get_ranked_local_values()
                #
                local_tabular_explanation = tabular_explainer.explain_local(X_test[0:5])
                local_tabular_features = local_tabular_explanation.get_ranked_local_names()
                local_tabular_features = local_tabular_explanation.get_ranked_local_values()
        # Create Explaination Client:
            # 
            from azureml.core.run import Run
            from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient
            from interpret.ext.blackbox import TabularExplainer
            #
            run = Run.get_context()
            explainer = TabularExplainer(model = loan_model, initialization_examples = X_train, features = features, classes = labels)
            explanation = explainer.explain_global(X_test)
            #
            explanation_client = ExplanationClient.from_run(run)
            explanation_client.upload_model_explanation(explanation, comment = 'Tabular Explanation')
            run.complete()
            # View the explanation:
            from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient
            explanation_client = ExplanationClient.from_run_id(workspace = ws, experiment_name = experiment.experiment_name, run_id = run.id)
            model_explanation = explanation_client.download_model_explanation()
            feature_importances = model_explanation.get_feature_importance_dict()
        # Visualize Explanations:
            # Visualization are only available for experiment runs that were configurated to generate and upload explanations.
                # When using Automated ML, only the run producing the best model has explanations generated by default.
            # Visualizing Global Feature Importance:
            # Visualizing Local Feature Importance:
            # Visualizing Summary Importance:
            
    # Manage Models: 
        # Potential Cause of Disparity:
            # Cause 1: Data Imbalance:
                # Some groups may be over/under-represented in the training data, or the data may be skewed so that it does not represent the overall population.
            # Cause 2: Indirect Correlation:
                # 
            # Cause 3: Societal Biases:
                #
        # Mitigating Biases:
            # Option 1: Balancing training and validation data: (e.g. over/under-sampling, stratification in getting train and test dataset, etc.)
            # Option 2: Perform extensive feature selection and engineering analysis: 
            # Option 3: Evaluate models for disparity based on significant features:
            # Option 4: Trade-off overall predictive performance for the lower disparity in predictive performance between sensitive feature groups.
                # e.g. a model with 95% accuracy in all groups might be better compared to a model with 99.9% accuracy only in 1 group.
            # Mitigate unfairness with Fairlearn:
                # 
        # Data Drifting:
            # Create a Data Drift Detector:
                from azureml.datadrift import DataDriftDetector
                monitor = DataDriftDetector(workspace = ws, name = 'Data Drift Detector', baseline_data_set = train_ds, target_data_set = new_data_ds,
                                                            compute_target = 'clusterwx1', frequency = 'Week', feature_list = ['age', 'height', 'bmi'], latency = 24)
                # After creating the dataset minitor, you can backfill to immediately compare the baseline data to existing data in the target dataset:
                    import datetime as dt
                    backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks = 6), dt.datetime.now())
            # Configurate data drift monitor schedules and alerts:
                alert_email = receiver@gmail.com
                monitor = DataDriftDetector.create_from_datasets(ws, 'Data Drift Detector1', baseline_data_set, target_data_set, 
                                                                                             compute_target = cpu_cluster, frequency = 'Week', latency = 2, 
                                                                                             drift_threshold = 0.3, alert_configuration = alert_email)
######################################################################################################
# 4. Deploy and Consume Models (create compute production target, deploy model as a service, create pipeline, publish pipeline as a web service)
######################################################################################################
# Step 1: Save data transformations:
# Step 2: Save trained model: (need to save the model in a serialized/ binary format)
    # Model/ Object Serialization: serialize objects/ attributes/ functions and deserialize them for future usage.
        # (Serialize = transform into a format, e.g. binary, that can be stored) (can use Joblib/ Pickle/ dill, etc.)
# Step 3: Register models and Transformations:
# Step 4: Create Azure Kubernete Service (AKS) Cluster:
# Step 5: Create a production environment:
# Step 6: Create inference configuration: (create entry script, environment, inference configuration)
# Step 7: Create an entry script to retrieve models and transformations for predictions:
# Step 8: Deploy the model and consume the web service: 
    #
    # Create Pipeline:
        # Step 1: Register a new Azure Storage file container datastore.
        # Step 2: Create a PipelineData object, Specify a name and output datastore.
        # Step 3: Specify a PipelineData object for data output.
            # General pipeline procedure include 2 major stages: (create_pipeline.py)
                # Data Processing: reading data, select/drop columns, replace missing values, normalize data, upload the data and log metrics (data_prep.py)
                # Build and Train Model: read data, split into train and test, train and test the model, upload results and metrics (train_model.py)
        from azureml.pipeline.core import Pipeline
        steps = 
        pipeline = Pipeline(workspace = ws, steps = steps)
        pipeline_run = experiment.submit(pipeline, continue_on_step_failure = True, 
                                               regenerate_outputs = True, pipeline_parameters = {'param1': 'value1'})
    # Real-time Inference Pipeline: (the 3 steps only applies when you create a inference pipeline from an existing normal trained model)
        # Step 1: Remove target variable from "Select Columns in Dataset" Module
        # Step 2: Directly (change) connect "Web Service Input" with "Apply Transformation" (2nd Input)
        # Step 3: "Evluate Model" is not needed as we cannot test on new data input(therefore removed).
    # Batch Inference Pipeline:
        # 
    # Publish Pipeline:
        # Option 1: call publish method:
            published_pipeline = pipeline.publish(name = 'training_pipeline', description = 'Model Training Pipeline', version = '1.0')
        # Option 2: Call the publish method on a successful run of the pipeline:
            pipeline_experiment = ws.experiment.get('training-pipeline')
            run = list(pipeline_experiment.getruns())[0] # naturally from most recent to earlier runs
            published_pipeline = run.publish_pipeline(name = 'training_pipeline', description = 'Model Training Pipeline', version = '1.0')
    # After the pipeline has been published, you can view it in Azure ML Studio, you can also determine the URI of its endpoint:
        rest_endpoint = published_pipeline.endpoint
        print(rest_endpoint)
    # Using a published pipeline: (REST API request)
        import requests
        response = requests.post(rest_endpoint, header = header, json = {'Experiment_Name': 'run_trained_pipeline', 
                                                                         'ParameterAssignments': {'learning_rate': 0.1}})
        run_id = response.json()['id']
        print(run_id)
    # Schedule Pipeline for periodic intervals/ data change:
        # Option 1: time frequency:
            from azureml.pipeline.core import ScheduleRecurrence, Schedule
            weekly = ScheduleRecurrence(frequency = 'week', interval = 1)
            pipeline_schedule = Schedule.create(ws, name = 'Weekly Run Predictions', description = 'batch inferencing', 
                                                    pipeline_id = published_pipeline.id, experiment_name = 'Batch Prediction', recurrence = weekly)
        # Option 2: based on data change:
            from azureml.pipeline.core import Schedule
            from azureml.core.dataset import Datastore
            #
            datastore = Datastore(workspace = ws, name = 'workspaceblobstore')
            schedule = Schedule.create(workspace = ws, name = 'Sample Schedule Name', pipeline_id = 'pipeline_id', 
                                       experiment_name = 'experiment name', datastore = datastore, 
                                       pooling_interval = 5, path_on_datastore = 'file/path') # the datastore is pulled every 5 minutes to check for any changes.
    # Register a trained Model:
        # Option 1: use the register method
            from azureml.core import Model
            register_model = Model.register(workspace = ws, model_name = 'name of your model', model_path = 'trained_model.pkl', description = 'classification model')
        # Option 2: refer to runs that have been used to train the model:
            run.register_model(model_name = 'classification_model1', model_path = 'outputs/model1.pkl', # this is the run output path
                                                                     description = 'a classification model')
    # Deploy the trained Model:
        #
        from azureml.core.model import Model
        model = ws.models['classification_model1']
        service = Model.deploy(workspace = ws, name = 'service name', models = [model], 
                                                                      inference_config = classifier_inference_config, 
                                                                      deployment_config = classifier_deploy_config, 
                                                                      deployment_target = production_cluster)
        service.wait_for_deployment(show_output = True) # for ACI or local service, you can omit show_output parameter or set 'show_output = None'
        #
        from azureml.core.conda_dependencies import CondaDependencies
        from azureml.core.webservice import AciWebservice
        from azureml.core.model import InferenceConfig
        #
        my_environment = CondaDependencies()
        my_environment.add_conda_package('scikit-learn')
        #
        folder_name = ''
        environment_file = folder_name + '/diabetes_env.yml'
        with open(environment_file, 'w') as f:
            f.write(my_environment.serialize_to_string())
        #
        inference_configuration = InferenceConfig(runtime = 'python', source_directory = folder_name, entry_script = 'score_diabetes.py')
        deployment_configuration = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, token_auth_enabled = True)
    # Consuming a real-time inferencing Service:
        # Option 1: using Azure ML SDK:
            import json
            new_input = [[0.1, 0.2, 0.3, 0.4], [1.1, 1.2, 1.3, 1.4]]
            json_data = json.dumps({'data': new_input})
            response = service.run(input_data = json_data)
            predictions = json.loads(response)
            for i in range(len(new_input)):
                print(new_input[i], predictions[i])
        # Option 2: Using a REST endpoint:
            #
            import requests
            endpoint = service.scoring_uri
            #
            request_header = {'Content-Type': 'application/json', 
                              'Authentication': 'Bearer' + Key/ or Token}
            response = requests.post(url = endpoint, data = json_data, headers = request_header)
            predictions = json.load(response.json())
        # There are 2 kinds of authentication that can be used:
            # Option 1: Key = requests are authenticated by specifying the key associated with the service
                # ACI only supports key authentication.
                primary_key, secondary_key = service.get_keys()
                #
                AksWebservice.deploy_configuration(auth_enabled = True)
                #
            # Option 2: Token = requests are authenticated by providing a JSON Web Token (JWT)
                #
                (Token authentication and Key-based authentication cannot be enabled at the same time.)
                AksWebservice.deploy_configuration(token_auth_enabled = True, auth_enabled = False)
                #
        # Check the Service State:
            from azureml.core.webservice import AksWebservice
            service = AksWebservice(name = 'service name', workspace = ws)
            print(service.state) # check the service state
            print(service.get_logs()) # review the service log
######################################################################################################
# Azure ML Workspace (Architecture/ Structure)
######################################################################################################
# Assets within ML Studio: (based on UI logic):
# Azure ML Studio = web-based platform using which users could track details about the ML appplication
# Azure ML Workspace = boundary of related assets used in ML Studio.
    # Notebooks:
    # Automated ML:
        from azureml.train.automl import AutoMLConfig
        automl_run_config = RunConfiguration(framework = 'python')
        automl_config = AutoMLConfig(name = 'Automated ML Experiment', task = 'classification/ regression', primary_metric = 'AUC_Weighted',
                                     compute_target = computewx1, training_data = train_dataset, validation_data = test_dataset, 
                                     label_column_name = 'Y', featurization = 'auto', iterations = 12, max_concurrent_iterations = 4)
        # Step 1: Specify Data for Training:
        # Step 2: Specify the Primary Metrics
            from azureml.train.automl.utilities import get_primary_metrics
            get_primary_metrics('classification')
        # Step 3: Sumbit an automated ML Experiment: (could specify early termination based on accuracy as well)
            from azureml.core.experiment import Experiment
            automl_experiment = Experiment(ws, 'automl_experiment1')
            automl_run = automl_experiment.submit(automl_config)
        # Step 4: Retrieve the best run and the corresponding model. 
            best_run, fitted_model = automl_run.get_outputs()
            best_run_metrics = best_run.get_metrics()
            for metric_name in best_run_metrics:
                metric = best_run_metrics[metric_name]
                print(metric_name, metric)
    # Designer:
        # The submit button runs the pipeline against the dataset provided.
        # The publish button creates a REST endpoint to the pipeline that other users/ developers/ data scientists can make calls to.
            # It also provides an endpoint with a key-based authentication.
            # (publish button will not deploy the model as a web service endpoint)
    # Datasets:
    # Experiments:
        # Note: When 2 experiment has the same name, they will be classified under the same experiment, just different runs for the same experiment.
            # e.g. from Run 1 to Run 2 (previous output will not be impacted)
    # Pipelines = ML Workflow
        # pipelines are created for the purpose of automating ML workflow
            # Step 1: Create the environment
            # Step 2: assign compute clusters
            # Step 3: create data transfer folder
            # Step 4: define pipeline steps
            # Step 5: build the pipeline
            # Step 6: create/ access an experiment
            # Step 7: run the pipeline
            #
            from azureml.pipeline.core import Pipeline, PipelineData
            from azureml.steps import PythonScriptStep
            #
            datastore = ws.get_default_datastore()
            process_step_output = PipelineData('processed_data', datastore = datastore)
            process_step = PythonScriptStep(script_name = 'python_process.py', arguments = ['--data_for_train', process_step_output], 
                                            outputs = [process_step_output], compute_target = computewx1, source_directory = process_directory)
            train_step = PythonScriptStep(script = 'python_step.py', arguments = ['--data_for_train', process_step_output], 
                                          inputs = [process_step_output], compute_target = computewx1, source_directory = train_directory)
            pipeline = Pipeline(workspace = ws, steps = [process_step, train_step])
            #
            # Forcing all steps in a pipeline to re-run
                pipeline_rerun = experiment.submit(train_pipeline, regenerate_outputs = True) # regenerate_outputs means running all steps again
            # Create a pipeline with a ParallelRunStep:
                experiment_folder = ''
                parallel_run_configuration = ParallelRunConfig( source_directory = experiment_folder, entry_script = 'batch_inference.py', 
                                                                mini_match_suze = '5', error_threshold = 10, output_action = 'append_now', 
                                                                environment = batch_env, compute_target = inference_cluster, node_count = 2)
                from azureml.pipeline.steps import ParallelRunStep
                from azureml.pipeline.core import Pipeline
                from azureml.core.experiment import Experiment
                #
                parallel_run_step = ParallelRunStep(name = 'preduct-infections', parallel_run_config = parallel_run_configuration, 
                                                    inputs = [input_infections_ds_consumption], output = output_dir, allow_reuse = True)
                #
                pipeline = Pipeline(workspace = ws, steps = [parallel_run_step])
                experiment = Experiment(ws, 'infection_identification')
                pipeline_run = experiment.submit(pipeline)
                # Download the output files that are generated by the experiment to a local folder:
                import tempfile
                batch_run = pipeline_run.find_step_run(parallel_run_step.name)[0] # get the last execution of the parallel run steps from the pipeline
                batch_output = batch_run.get_output_data(output_dir.name) # read the output data in batch_output object
                target_dir = tempfile.mkdtemp() # create a local target directory
                batch_output.download(local_path = target_dir) # download the output file
                #
            # Run the pipeline and retrieve the Step Output:
                # 
    # Models:
        # To deploy the model as a web service endpoint, you need to navigate to Model menu on ML Studio, select the model and deploy.
    # Endpoints:
    # Compute: 
        # Compute Insatances:
            # (local) work station for data scientist (cannot adjust computing power automatically)
                # local computer training target supports AutoML, whereas does not support ML Pipelines or Azure ML designer.
                # Do not provide GPU-based instance configuration.
        # Compute Clusters: (2 Nodes --> 2 Virtual Machine/ instance currently provisioned to the cluster)
            # (distributed, can automatically increase/ decrease computer capacity based on demand)
                # Supports AutoML, ML Pipeline, Azure ML Designer.
                # Used for batch inferencing, not suitable for real-time inferencing.
        # Inference Clusters: (deploy your model to Azure Kubernetes Service for large scale inferencing)
            # Azure Kubernetes Service (AKS) cluster, then deploy model as a REST Endpoint. (cannot adjust computing power automatically)
                # In ML, inferencing refers to the use of trained model to predict labels for new data on which models has not been trained.
                    # AKS compute is for deployment only in terms of ML tasks.
        # Attached Compute: 
            # An existing VM, Databricks, Cluster, etc., that can be borrowed here as compute resource.
                # Remote VM does not support Azure ML Designer.
    # Datastores:
        # Azure Blob Storage:
        # Azure Data Lake: 
        # Azure SQL/ PostgreSQL/ MySQL Database:
        # Azure File Share: 
    # Data Labelling:
    # Linked Services:
        # for connecting to other Azure Services, e.g. Synapse workspace, and managed linked assets in 1 place.
######################################################################################################
# Managed Resources (compute instance/ cluster)
# Dependencies (Azure Storage Account, Azure container registry, Azure Key Vault, Azure Application Insights)
    # Azure Storage Account:
        # Containers:
        # File Shares:
        # Tables:
        # Queues: 
    # Azure Application Insights:
        # Application insights is an application performance management service in MS Azure that enables the capture, storage, and analysis
            # of telemetry data from applications.
        # Check Application Insights resource associated with the Workspace:
            from azureml.core import Workspace
            ws = Workspace.from_config()
            ws.get_details()['applicationInsights']
        # Enabling Application Insights for an application (could be external to Azure):
            
            ment_configuration = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 2, enable_app_insights = True)
            # Enable/ or modify the deployment configuration for Azure Kubernetes Service (AKS) based services:
                service = ws.webservices['my-service1']
                service.update(enable_app_insights = True)
        # Write Log data:
            def init():
                global model
                model = joblib.load(Model.get_model_path('my_model_location'))
            def run(new_data):
                data = json.loads(new_data)['data']
                predictions = model.predict(data)
                log_text = 'Data: ' + str(data) + ' - Predictions: ' + str(predictions)
                print(log_text)
                return predictions.to_list()
        # Query logs in Application Insights:
            # 
######################################################################################################
# General Azure ML Concepts:
######################################################################################################
#
# AI (Artificial Intelligence) = computer performing tasks without being specifically programmed to.
# CUDA (Compute Unified Device Architect) 
# 
# Azure ML service stores metrics and metadata in Azure Cosmo DB instance where all data is encrypted at rest. 
# 
# Azure Container Instance (ACI): 
    # Containers are becoming the preferred way to package, deploy, and manage cloud applications.
    # Azure Container Instance is a great solution for any scenario that can operate in isolated containers, including simple applications,
        # task automation, and build jobs.
            # For scenario where you need full container orchestration, including service discovery across multiple containers, automatic sacling,
                # and coordinated application upgrades, Azure Kubernetes Service (AKS) is recommended.
    # Azure Container Instance (ACI) are well suited for testing/ debugging workloads and only support low-scale CPU-based workloads.
        # Do not provide GPU-based instance configuration.
# Azure Container Registry (ACR): 

# Transport Layer Security (TLS):
# Secure Sockets Layer (SSL): 

# Apache Ambari: 
# Endpoint
# Mimic Explainer:
# Tabular Explainer:
# PFI Explainer:

# Inference Cluster
# Permutation Feature Importance:
# Mutual Information:
# Mood's Median Test:
#
# IoT Edge Module:
    # Azure Stack Edge: Azure Stack Edge is a hardware-as-a-service platform offered by Microsoft.
        # filter, aggregate, and optimize data in order to reduce bandwidth requirements.
#
# Service Principal (SP): 
    # Use the client secrets to retrieve authentication token
#
# Azure ML Experiment = named process, usually the running of a script or pipeline, that can generates metrics/ outputs.
    # and can be tracked in the Azure ML workspace
# 
# Azure Cognitive Service
# Azure Data Lake Analytics:
    # Azur Data Lake Analytics is an on-demand analytics job service that simplifies big data.
    # Azure Data Lake Analytics is for easy develop and run massively parallel data transformation and processing programs in U-SQL, R, Python, and .Net over
        # petabytes of data. With no infrastructure to manage, you can process data on demand, scale instantly, and only pay per job.
# Azure HDInsight with Spark Mlib
# Azure HDInsight:
    # 
#
# Differential Pricacy:
    # Differential Privacy seeks to protect individual data value by adding "noise" to the analysis process.
        # Epsilon Value: ( 0-1 ) --> (Privacy - Accuracy)
######################################################################################################
# Cloud Computing:
    # What is Cloud Computing:
        # Cloud computing is the on-demand availability of computer system resources, especially data storage(cloud storage) and computing power,
            # without direct active management from the user (no pyhsical infrastructure management).
    # Why do we need Cloud Computing:
        # If the demand of computing power fluctuates significantly.
        # Save capital input for better cashflows.
        # Save time to get the infrastructure ready.
    # Cloud Service include: (3 main categories)
        # Infrastructure as a Service (IaaS):
            # e.g. Virtual Machine, Servers, Storage, etc.
        # Platform as a Service (PaaS):
            # e.g. Machine Learning Studio (as a platform)
        # Software as a Service (SaaS): 
            # e.g. Office 365, Google Sheet, etc.
######################################################################################################
# Azure Fundamental Concepts: (ref. AZ 900)
######################################################################################################
# What is Azure: (Project Red Dog, Oct 2008 --> Windows Azure, Feb 2010 --> Microsoft Azure, Mar 2014)
    # Azure is a Cloud computing service created by Microsoft for building, testing, deploying and managing applications and services,
        # through Microsoft-managed data centers.
        # Azure include many different resources include but not limited to:
            # Azure Compute:
                # e.g. Virtual Machine, Virtual Desktop, etc.   
            # Azure Networking Resources:
                # e.g. managing firewalls, web traffic, etc.
            # Azure Storage:
                # e.g. HDD, SSD, etc.
            # Azure Database:
            # Azure Analytics:
            # Azure AI/ML:
            # Azure Identity:
                e.g. Azure Active Direcory
            # Azure Security:
            # Azure DevOps:
            # Azure IoT, Mobile, Containers, Blockchain, Migrate, etc.
######################################################################################################
# Key Terms and Concepts of Azure:
    # Management Groups:
        # Under each Management Group, there might be multiple subscriptions:
            # e.g. some are free subscriptions, some are Pay-as-you-go subscriptions.
    # Subscriptions:
    # Resource Groups:
        # Under each subscription, there could be multiple resource groups.
        # Resource Group are essentially a "container" that gets all the needed resources together, or into a group.
        # It is recommended that resources that supports the same application, or in the same life cycle, to be put under the same resource group.
            # so that we could standardize the process of creation, deployment, deletion of resources.
            # also helps the security, manage the accessibility, record log of manipulation by users, etc.
    # Resources:
        # e.g. SQL Server Database, ML Studio, Blob, etc.
    # Data Centers:
        # Physical buildings that house networked computer servers.
    # Regions:
        # Set/ Cluster of datacenters.
    # Availability Zone:
        # Physically spearated set of data centers within a region for high availability.
######################################################################################################
# Azure Storage:
    # Types:
        # Azure Disks:
        # Azure Blobs:
            # Object Storage Solution
            # Ideal for massive unstructured data such as text and images
            # Use containers of data
        # Azure File Share: (typically used to sharing files with end users)
            # Provide file share as a drive.
            # Supports SMB or NFS file format.
            # Can be used as back-up of various application files.
        # Azure Queues: (data from multiple users at a time)
            # For storing and retrieving the messages.
            # Usually, the messages are stored as a list and then processed.
        # Azure Tables: (upgraded to Azure Cosmos DB)
            # NoSQL Tables (data are structured/ in tabular format, whereas not correlated)
            # Uses the name-value pair for storing and retrieving the data
            # Offers high scalability and flexibility (because the name-value pair structure)
    # Access Tiers:
        # Hot: Frequently accessed data
            # e.g. saved on SSD
        # Cool: Less Frequently accessed data.
            # e.g. saved on HDD
        # Archive: Rarely accessed and stored offline.
            # e.g. storage not connected to the network/ server on most occassion.
    # Redundancy:
        # Locally Redundant:
            # e.g. 3 copies are in the same pyhsical location, just different machines.
            # least expensive.
        # Zone Redundant:
            # e.g. 3 copies in different availability zones.
        # Geo Redundant:
            # e.g. 3 copies in single pyhsical location in primary region + 1 copy in secondary region
        # Geo-zone Redundant:
            # e.g. 3 copies in 3 zones in primary region + 1 copy in secondary region
    # Performance Tiers:
        # Standard:
        # Premium: Low delay, high availability.
######################################################################################################
# Azure Compute:
    # Azure Virtual Machine Trypes:
        # A-Series = Entry Level, economical VM for development and testing.
        # D-Series = General Purpose Compute.
        # F-series = CPU-intensive workloads
        # L-series = storage optimized
        # N-Series = GPU Enables Virtual Machines
        # e.g. you could have multiple Virtual Machines on the same server (which has sufficient computing power)
######################################################################################################
# Docker:
    # Docker is a tool to ship a group of software packages using containers.
        # Container is a bundle of software, libraries, dependencies, configuration files, etc.
        # To improve portability, we usually create environments in Docker containers that are in turn be hosted in compute targets,
            # such as your development computer, virtual machine, or clusters in the cloud.
    # Docker Image is a template or specifications.
        # e.g. 1 docker image would include: Windows OS version, Python Version, Pandas Version, Scikit-learn version, etc.
        # Docker Image is like a class, and a container is like an instance of that class.
            # We could create multiple containers using (running) 1 docker image.
    # Once created, user could not modify docker files (a series of read-only files).
        # Docker files could be created and shared.
        # e.g. in the 5 stages of software development: Gather Requirements, Design, Development, Testing, Deployment:
            # The last 3 steps could (should) use the same docker file.
    # Note: when using GPU for inferencing, local web service is not supported.
######################################################################################################
# Outliers: observations that have much higher distance compared to other observations
    # outliers does impact the prediction/ estimation of the sample/ model
    # outliers could result from human input error, malfunction of measurement equipment, data transmission error, system bebaviour eror,
        # feaudulent behaviour, natural outliers, sampling error, etc.
    # boxplot, histogram and scatterplot are recommended to identify outliers visually
        # as well as percentile analysis (most common method is visualization)
#    
# Variables at different scale
    # Normalize the dataset
######################################################################################################
# Too many variables or curse of dimensionality (100 or 1000 variables)
    # too sparse data and low accuracy
    # require high run time
    # may lead to over-fitting
        # Principal Component Analysis (PCA)
            # Create a new set of coordinates for the data
            # Reveals the internal structure of the data that best explains the variance in the data
            # Reduce the dimensionality 
                # Eigenvectors
                # Eigenvalues
######################################################################################################
# Missing Value in a large dataset
    # replace with MICE
# Unbalanced Dataset: presence of minority class in the dataset (e.g. 0.5% fraud, 99.5% without fraud data)
    (e.g. manufacturing defects, rare disease disgnosis, etc.)
    # might have biased predictions
    # might have misleading accuracy
    # Need to either decrease the majority (e.g. Random Under-Sampling) class or increase the minority class (e.g. SMOTE)
        # If we are simply duplicate the minority class, that is likely to cause over-fitting problem.
        # Synthetic Minority OverSampling Technique (SMOTE)
            # Step 1: Identify the feature vector and its nearest neighbor
            # Step 2: Take the difference between the 2 (feature vector and its nearest neighbor)
            # Step 3: Multiply the difference with a random number between 0 and 1
            # Step 4: Identity a new point on the line segment by adding the random number to the feature vector
            # Step 5: Repeat the process for identified feature vectors.
######################################################################################################
# Merge datasets on column values
    # join data (datasets are related by key columns)
        # types of joins:
            # Inner Join: select only records that exist in both tables
            # Left Outer Join: select all rows in the left table, and some rows from the right table with matched keys
            # Full Outer Join: select all the rows from both tables
            # Left Semi Join: select left table rows and columns only, and the rows must have mathing keys in the right table
# 
######################################################################################################
# Azure Identity Servive
    # Authentication (credentials, such as username + password) = prove who you are
    # Authorization (grant access based on permission) = what you are allowed to do based on who you are
######################################################################################################
# Azure Active Directory (ID as a service/ IDaaS)
    # when the service involve both cloud and on-premise, or through external internet, there is not a central location to authenticate users.
        # 
    
# 
    # Single Sign on (SSO) = same access for multiple apps
    # Role-based access 
######################################################################################################
# Azure Resource Lock (Cannot delete or read only) = 
    # Locks are interited from the parent group/ resource
# 
 
 
######################################################################################################
# Azure Policy 
 
 
# Azure Blueprint 

######################################################################################################
# Gradient Descent
    # Cost Function
######################################################################################################
# Normalization: a method to standardize the range of independent variables or features of data
    # Variables are fitted within a certain range (generally between 0 and 1, e.g. min-max scaler)
    # applied only on numeric values
        # Option 1: Z-score
        # Option 2: Min-Max Scaler
        # Option 3: MaxAbs Scaler = scale feature by their maximum absolute value
        # Option 4: Logistic
        # Option 5: 
        # Option 6: StandardScaleWrapper = ensure that features are standardized by altering the dataset, removing the mean, and scaling to unit variance
# Accuracy
# AUC Weighted
# 
######################################################################################################
# Mean Deviation (when the number of observations is small)
# Standard Deviation (when the number of observations is large)
    # describe how the data is distributed around the mean
    # std(x) = [ sum_of_squared_distances/ (n-1) ] ^ 0.5
        # when the mean of 2 datasets are similar, std(x) could be comparable
# Quartile
    # 1st quartile, median, 3nd quartile
    # IQR = Inter Quartile Range = Q3 - Q1
# Skewness
    # Skewness the a measure of the asymmetry of the probablity distribution of a real-valued random variable about its mean.
        # Positive Skewness = Skewed to left, long tail to right = mode < median < mean
        # Negative Skewness = Skewed to right, long tail to left = mean < median < mode
# Kurtosis
    # 
 
######################################################################################################
# ML Concepts:
######################################################################################################
# Supervised Learning: 
    # Classification
        # Logistic Regression:
            # Step 1: Import Data
            # Step 2: Select Column
            # Step 3: Clean Missing Data
            # Step 4: Normalize Data/ Convert Some Column into Categorical Data/ One-Hot Encoding
            # Step 5: Split Data
            # Step 6: Train Model 
            # Step 7: Score Model
            # Step 8: Evaluate Model
        #
        # Decision Tree: (could be for both categorical and continuous variables)
            # Boosted Decision Tree: (for classification by regions)
                # 
            # Decision Forest:
            # Decision Jungle:
        #
    # Regression 
        # Linear Regression (Metrics)
            # MAE (Mean Absolute Error)
            # RMSE (Root Mean Squared Error)
                # punish for large errors
            # RAE (Relative Absolute Error): sum(actual_y - predicted_y)/ sum(actual_y - actual_average_y)
            # RSE (Relative Squared Error): sum(actual_y - perdicted_y)^2 / sum(actual_y - actual_average_y)^2
            # R-squared: (Coefficient of Determination) how many % of variation in y is explanined by the variation in x 
                # r2 = SSR/ SST
                    # SSR = sum of squared regression = sum(predicted_y - actual_y_mean)^2
                    # SST = sum of squared total = sum(actual_y - actual_y_mean)^2
            # : 
    # Decision Tree Regression: (divide all the input variables by region, and return: either mean value of that region, or regression line for each region)
        # Threshold
    #
    # 
######################################################################################################
# Unsupervised Learning: does not use previously known label values to train the model 
    # Clustering:
        # e.g. recommendaiton, market segmentation, social network analysis, image segmentation, detect anomaly, etc.
        # Euclidean Distance = 
    #
    # K-means Clustering: nominate the number of clusters (K) 
        # Centroid: center point of each cluster (each point is assigned to the nearest centroid) 
            # Each centriod is moved to the center of the data points based on the mean distance of all points within that cluster
            # After moving the centroid, some data points may be closer to a differetn centriod (therefore the cluster will be re-assigned for that data point)
            # The centroid movements stop until the clustering is stable or the maximum steps allowed is reached 
    # K-means++ (default in ML studio and an improvement over finding the initial means)
        # Step 1: the 1st cluster center is chosen uniformly at random from the datapoints
        # Step 2: each cluster center is chosen from the remaining datapoints with probablity propotional to its squared distance closest to existing cluster centers
    #
    # K-means++ Fast: (for faster clustering)
    # Evenly
    # 
    # 
    # SVM:
        # SVM target for 2 metrics:
            # Target 1: find a hyperplane with the largest minimum margin:
            # Target 2: find a hyperplace that correctly separate as many instances as possible:
                # The C-parameter determines how much you value the 2nd target, to separate the instances.
                    # Low C-parameter = the minimum margin is high (good), whereas the accuracy might be lower (bad).
                    # High C-parameter = the minimum margin is low (bad), whereas the accuracy is higher (good).
        # Accuracy is more important than the margins in SVM
        # 
    # 
# 
######################################################################################################
# Reinforcement Learning: 
    # 

    # 

#
######################################################################################################
# Feature Selection
    # for simplification of models so that they could be interpreted and understood more easily
    # for shorter training time
    # for better accuracy
    # for avoiding the curse of dimensionality
    # for enhanced generalization that helps reducing over-fitting
        # Filter-based Method: use the correlation of x1, x2, x3, ... xn to Y, and choose the most relevant/ influential x
            # Pearson Correlation Coefficient (from =1 to +1 )
                # may be impacted significantly by outliers
                # not good for non-linear relationship
        # Chi-squared Test of Independence:
            # evaluate the relationship between 2 categorical variables
                # Step 1: Define Null and Alternative Hypothesis
                    H0: there is no correlationship between the 2 variables (try to reject H0)
                    H1: alternative, there is correlation
                # Step 2: Define Alpha (probablity that H1 is true, e.g. alpha = 5%)
                # Step 3: Calculate the Degree of Freedom 
                    # DOF = (num of rows - 1) * (number of columns - 1) = (2-1) * (3-1) = 2
                # Step 4: State the Decision Rule:
                    # Critical value for chi-squared distribution
                # Step 5: Calculate the Test Statistics:
                    # 
                # Step 6: Results and Consultion:
                    # if chi-squared value exceeds the limit, reject H0
        # Kendall Correlation Rank Correlation:
            # 
            # e.g. Pearson Correlation, Chi-squared, Mutual Information, Fisher Score, etc.
        #
        # Wrapper Method: test with different combinations of x1, x2, x3, ... xn, and choose the combination of variables that has the best performance
            # e.g. accuracy/ precision or RMSE based on different models
            # Forward Selection: add 1 variable after 1 variable
            # Backward Elimination: eliminate 1 variable after 1 variable
            # Recursive Feature Elimination: random select variables in each iteration, keep the most important variables and removes least influential ones.
        # Embedded Method: 
######################################################################################################
# Impact of Split Percentage:
# Impact of Stratification:
# Low L1 and High L2:
# High L1 and Low L2:
# High L1 and High L2:
# Low L1 and Low L2: 
######################################################################################################
# Recommendation System: predict the rating/ preference that a customer would give to a new item
    # Problem: cold start, same scale judgement (users not having same standard)
    # 
    # Collaborative Filering: (need user preference and much user activities data)
        # Similar action is assumed for people with similar taste/ preference in the past.
    # Content-base Filering: (need item features + need user to like a few items)
        # Similar feature within the items.
    # Popularity Based Filering:
        # Simply recommend what is generally popular.
    # Hybrid System:
        # Matchbox Recommendation System: 
    #
    # Recommender Split:
    #
######################################################################################################
# Azure ML Matchbox Recommender System:
    # Based on the Hybrid approach of content and collaborative filering technique
        # When customers are new, rely on item feature
        # When customers have sufficient activities, use activities as well
    # Require user-item-rating triples and, optionally, somer user and item features.
        # First Input: (mandatory)
            # Column 1: User Identifier
            # Column 2: Item Identifier
            # Column 3: Rating for the User-Item pair (has to be either numeric or categorical)
        # Second Input: (Optional)
            # User ID and the user Features (1st column must be User ID if not blank here)
        # Third Input: (Optional)
            # Item ID and Item features (1st column must be Item ID if not blank here)
    # Number of Traits:
        # How many traits to learn for each user and item
        # Each feature is associated with a latent "trait" vector
    # Number of Recommendation Algorithm Iterations:
        # How many times that the algorithm should process the data
            # recommended 5-10
    # Number of Training Batches:
        # Number of batches for dividing the data into during training
#
######################################################################################################
# Natural Language Processing:
######################################################################################################
    # Step 1: Noise Removal:
        # Stopwords, Stemming (lemma), Upper/Lower case, numbers, special characters (%, $, etc.). duplication, email, url, etc.
        # Need to consider separation in sentence.
        # Tokenization (e.g. did not = did't)
        # Part-of-speech (e.g. build != building)
    # Step 2: Convert into Feature Vectors: (may encounter Curse of dimensionality)
        # Feature Engineering: 
            # Bag of words:
                # based on the occurrance of words
            # N-gram model: (instead of a single word, get 2 words, 3 words as basic components in the feature selection)
                # e.g. Machine ____ (would recommending learning if "leaning" has the highest chance of being mentioned) (based on probablity)
    # Step 3: Train, Score and Deploy Model:
######################################################################################################
#
######################################################################################################
# Azure AutoML:
######################################################################################################
    # Step 1: bring in dataset.
    # Step 2: define goal, constraints, etc.
    # Step 3: auto ML will test available algorithms, compare the accuracy and provide the evaluations.
        # Auto ML only accepts tabular data form.
#
#
    # Replace using MICE
    # MICE: multi-variate imputation using chained equation or multiple imputation by chained equations
        # Clean the missing value without affecting/ reducing the dimensionality of the feature set.
        # 
        # assuming that the data is missing randomly (not systematically or biased to a reason)
        # Step 1: calculate the mean value based on available data
        # Step 2: replace all missing value with mean
        # Step 3: choose dependent column and restore original (for the 1 column that you would like to fill/ solve)
        # Step 4: apply transformation and create prediction model
        # Step 5: predict missing value and repeat step 3-5 (for other columns with missing value)
            # e.g. mean for numeric, mode for string
######################################################################################################
Feature Selection:
######################################################################################################
    # Method 1: Filter Based Feature Selection:
    # Method 2: Principal Component Analysis: (unsupervised) (select the principal components based on the spread of the data)
        # Select Columns:
        # Number of dimensionality to reduce to:
            # Normalize dense dataset to zero mean
    # Method 3: Fisher Linear Discriminant Analysis
        # find an axis based on which certain threshold on that axis could separate the values into different groups well.
            # Analysis of Variance (ANOVA)
######################################################################################################
# Latent Direchlet Transformation:
    # Topic Modelling:
# 
# Build Counting Transformation Module:
#

#
# Replace Discrete Values:
    # 
#
#

#

######################################################################################################
# Hyperparameters Tuning/ Tune Model Hyperparameters : (every module has a set of input parameters, called hyperparameters)
    # helps in determining the best possible combination of model parameters to achieved a desicred result
        # Discrete Hyperparameters: (qnormal, quniform, qlognormal, qloguniform)
        # Continuous Hyperparameters: (normal, uniform, lognormal, loguniform)
            # Define a Search Space:
                from azureml.train.hyperdrive import choice, normal
                param_space = {'--batch_size': choice(16, 32, 64), '--learning_rate': normal(10, 3)}
            # Grid Sampling: (only applies to discrete values)
                from azureml.train.hyperdrice import GridParameterSmpling 
                param_sampling = GridParameterSampling(param_space)
            # Random Sampling:
                from azureml.train.hyperdrive import RandomParameterSampling, choice, normal
                param_space = {'--batch_size': choice(2, 4, 6, 8, 10), '--learning_rate': normal(10, 3)}
                param_sampling = RandomParameterSampling(param_space)
    # Optimization Tolerance: when a new iteration does not decrease the loss function (e.g. delta loss < 10^-5 )
    # L1 Regularization: (Lasso Regression)
        # Lasso regression can shrink some coefficients to zero, which means performing variable selection
    # L2 Regularization: (Ridge Regression)
        # Ridge regression will shrink all the coefficientby the same scale, which will not eliminate any variable
#
######################################################################################################
# Convert to ARFF: 
    # Convert datasets and results in Azure ML to attribute-relation file format (ARFF) used by the Weka toolset.
    # The ARFF data specification for Weka supports multiple ML tasks, including data preprocessing, classification, and feature selection.
        # In ARFF format, data is organized by entities and their attributes, and is contained in a single text file.
######################################################################################################
######################################################################################################
# Azure ML Studio Modules (should practice 3+ times)
######################################################################################################
# Data Input and Output Modules: (3)
######################################################################################################
    # Enter Data Manually:
        # Filter 1: Data Format:
            # CSV, ARFF, SvmLight, TSV
        # Filter 2: Has Header
    # Export Data:
        # Filter 1: Datastore type:
            # Azure Blob Storage, Azure SQL Database, Azure Data Lake Storage, Azure File Share.
    # Import Data:
        # Filter 1: Data Source:
            # Datastore:
            # URL vis HTTP:
######################################################################################################
# Data Transformation Modules: (19)
######################################################################################################
    # Add Columns:
    # Add Rows:
    # Apply Math Operations:
    # Apply SQL Transformation:
    # Clean Missing Data:
        # Filter 1: Columns to be Cleaned:
        # Filter 2: Minimum Missing Value Ratio:
        # Filter 3: Maximum Missing Value Ratio:
        # Filter 4: Cleaning Mode:
            # Option 1: Custom Substitution Value:
            # Option 2: Replace with Mean/ Median/ Mode.
            # Option 3: Remove entire Row/ Column.
    # Clip Values:
        # refer to df.clip() in pandas
        # Filter 1: Set of thresholds:
        # Filter 2: Upper Thresholds:
        # Filter 3: Constant Value for Upper Threshold
        # Filter 4: Substitute value for Peaks:
        # Filter 5: List of Columns:
        # Filter 6: Overwrite Flag:
        # Filter 7: Add Indicator Columns:
            # set of threshold
    # Convert to CSV:
    # Convert to Dataset:
    # Convert to Indicator Values:
        # Use the Convert to Indicator Values module to convert columns that contain categorical values into a series of binray indicator columns
            # that can be used as a series of features in the ML model.
            # (similar to one-hot encoding)
    # Edit Metadata: (Make string variable to be categorical)
        # Filter 1: Select Column
        # Filter 2: Select Data Type
        # Filter 3: Categorical
        # Filter 4: Fields
        # Filter 5: New Column Name
    # Group data into Bins:
        # Filter 1: Binning Mode: (Quantiles, Equal Width, Custom Edges)
        # Filter 2: Number of Bins:
        # Filter 3: Quantile Normalization (Percent, PQuantile, Quantile Index)
        # Filter 4: Columns into Bin
        # Filter 5: Output Mode (Append, Inplace, Result Only)
        # Filter 6: Tag Columns as Categorical (True/ False)
    # Join Data:
        # Filter 1: Selected Key for Left Table:
        # Filter 2: Selected Key for Right Table:
        # Filter 3: Match Case:
        # Filter 4: Join Type:
        # Filter 5: Keep right Key columns in the joined table:
    # Normaliza Data:
        # Filter 1: Transformation Method:
            # Z-Score, MinMax, Logistic, LogNormal, Tanh
        # Filter 2: Use 0 for constant columns when checked: True/ False
        # Filter 3: Columns to Transform: (numeric only)
    # Partition and Sample:
        # Expected Input = Data Table
        # Expected Output = Data Table
        # Filter 1: Partition or Sample Mode:
            # Assign to Folds, Pick Fold, Sampling, Head
        # Filter 2: Rate of Sampling:
            # e.g. 0.01 = 1%, 0.70 = 70%
        # Filter 3: Random Seed for Sampling
            # default to be 0
        # Filer 4: Stratified split for Sampling
            # True/ False
            # Sampling is an important module in ML as it lets you reduce the size of a dataset while maintaining the same ratio of values.
                # Dividing your data into multiple subsections of the same size.
                # e.g. you might use the partition for cross-validation, or to assign cases to random groups
                # Separating data into groups then work with data from a specific group.
                # Sampling application:
                    # e.g. either by percentage with random sampling, choose a column to use for balancing the dataset, or perform stratified sampling, etc.
                # Creating smaller datasets for testing.
                    # e.g. to save processing time during testing
    # Remove Duplicate Rows:
    # SMOTE:
        # Filter 1: Selected Column: 
        # Filter 2: SMOTE percentage: 
            # e.g. 2000% = +20 times = 21 times in total
        # Filter 3: Number of nearest neighbors:
            # 
        # Filter 4: Random Seed: 
    # Select Columns Transform:
    # Select Columns in Dataset
    # Split Data:
        # Filter 1: Splitting Mode: 
            # split rows:
            # Regular Expression: 
            # Relative Expression:
        # Filter 2: Fraction of rows in the 1st split: 0.7 (70% train, 30% split)
        # Filter 3: Random Seed: 123 (default = 0)
        # Filter 4: Stratified Split: False
######################################################################################################
# Feature Selection Modules: (2)
######################################################################################################
    # Filter Based Feature Selection:
        # Filter 1: Operate on feature Columns only: (True/ False/ Null)
        # Filter 2: Target Column:
        # Filter 3: Number of Desired Features:
        # Filter 4: Feature Scoring Method:
            # Pearson's Correlation:
            # ChiSquared:
            # Spearman (relatively more reliable when having some, but not extreme outliers)
            # Kandall: (more reliable when having significant outliers)
                # corr(x1, y)
    # Premutation Feature Importance:
        # 
######################################################################################################
# Machine Learning Algorithms Modules: (19)
######################################################################################################
# Regression Models:
    # Boosted Decision Tree Regression:
    # Decision Forest Regression:
    # Fast Forest Quantile Regression:
    # Linear Regression:
        # Filter 1: Solution Method:
            # OLS
            # Online Gradient Descent
                # create trainer mode
                # learning rate
                # number of training epochs
        # Filter 2: L2 Regularization Weight
        # Filter 3: Include Intercept Term:
        # Filter 4: Random Seed
    # Neural Network Regression:
    # Poission Regression:
######################################################################################################
# Clustering Models:
    # K-Means Clustering:
        # Filter 1: Create Trainer Mode (single parameter)
        # Filter 2: Number of Centroids
        # Filter 3: Initialization (K-Means++, K-Means++ Fast, First N, Forgy Method, Evenly and by Use Label Column.)
            # Use Label Column: use a variable from the existing dataset to guide selection of the centroids.
        # Filter 4: Random Number Seed:
        # Filter 5: Metric: Euclidean, Cosine
        # Filter 6: Normalize Features: True/ False
        # Filter 7: Iterations: e.g. 1000
        # Filter 8: Assign Label Mode: (ignore label column, fill missing values, overwrite from closest to center)
######################################################################################################
# Classification Models:
    # MultiClass Boosted Decision Tree:
    # MultiClass Decision Forest:
    # MultiClass Logistic Regression:
    # MultiClass Neural Network:
    # One-vs-All MultiClass:
    # One-vs-One MultiClass:
    # Two-Class Average Perception:
    # Two-Class Boosted Decision Tree:
        # Create Trainer Mode: Single Parameter/ Parameter Range
        # Maximum number of leaves per Tree: e.g. [2, 8, 32, 128]
        # Minimum Number of samples per leaf: e.g. [1, 10, 50]
        # Learning Rate: e.g. [0.025, 0.05, 0.1, 0.2, 0.4]
        # Number of Trees Constructed:
        # Random Number Seed:
    # Two-Class Decision Forest:
    # Two-Class Logistic Regression:
    # Two-Class Neural Network:
        # Filter 1: Create Trainer Mode: Single Parameter/ Parameter Range
        # Filter 2: Hidden Layer Specification: Fully Connected Case
        # Filter 3: Number of Hidden Nodes:
        # Filter 4: Learning Rate: 
        # Filter 5: Number of Learning Iterations:
        # Filter 6: The Momentum: how does nodes in previous iteration affect current iteration
        # Filter 7: Shuffle examples:
        # Filter 8: Random Number Seed:
    # Two-Class Support Vector Machine:
######################################################################################################
# Model Training Modules: (4)
######################################################################################################
    # Train Model:
        # launch column selector: select target variable
    # Train Clustering Model:
    # Train PyTorch Model:
    # Trune Model Hyperparameters:
######################################################################################################
# Model Scoring and Evaluations Modules: (6)
######################################################################################################
    # Apply Transformation:
    # Assign Data to Cluster:
    # Cross Validate Model:
    # Evaluate Model:
        # Confusion Matrix:
            #               Predicted(True)                 Predicted(False)
            # Actual(True)  True Positive                   False Negative (Type 2 Error)
            # Actual(False) False Positive (Type 1 Error)   True Negative
            # 
            # Matrix: [[TP, FN], [FP, TN]]
            # Accuracy = (TP + TN)/ (TP + TN + FP + FN) = total number of accurate predictions/ total number of observations
            #
            # Precision = TP/ (TP + FP) = (Among those classified as True, % accurate) = TP/ (TP + Type 1 Error)
            # Recall = TP/ (TP + FN) = (For those that are actually True, % identified as True) = TP / (TP + Type 2 Error)
            # 
            # F1 Score = 2/(1/Accuracy + 1/Precision) = 2/ [(2TP + FP + FN)/TP] = 2TP/ (2TP + FP + FN) = 1/ (1 + FP/ 2TP + FN/ 2TP)
                # F1 Score = Weighted average (Harmonic Mean) of Precision and Recall = 2 * Precision * Recall / (Precision + Recall)
        # AUC = Area Under Curve (e.g. value = 0.8)
            # ROC = Receiver Operating Characteristics
            # ROC Curve = ( FPR on x-axis and TPR on the y-axis )
                # TPR = True Positive Rate = TP / (TP + FN) = Recall
                # FPR = False Positive Rate = FP / (FP + TN) = 
    # Score Model:
    # Score Image Model:
######################################################################################################
# Python Language Modules: (2)
######################################################################################################
    # Python Language Modules:
        #
    # Execute Python Script:
        # With Python, users could perform tasks that are not currently supported by existing ML Studio Modules.
            # e.g. Visualizing data with Matplotlib
            # e.g. Using python libraries to enumerate datasets and models in your workspace
            # e.g. reading, loading, and manipulating data froms sources not supported by the "Import Data" Module
######################################################################################################
# R Language Module: (1)
######################################################################################################
    # Execute R Script
######################################################################################################
# Text Analysis Modules: (7)
######################################################################################################
    # Convert Word to Vector:
    # Extract N-grams Feature from Text:
        # Filter 1: Select Text Column
        # Filter 2: Vocabulary Mode (Create, ReadOnly)
        # Filter 3: N-gram Size:
        # Filter 4: Weighting Function (Binary Weight, TF Weight, IDF Weight, TF-IDF Weight)
        # Filter 5: Minimum / Maximum Word Length
        # Filter 6: Minimum N-gram document absosulte frequency.
        # Filter 7: Maximum N-gram document ratio.
        # Filter 8: Normalize N-gram feature vectors (True/ False)
    # Feature Hashing:
        # Feature hashing is used for linguistics, and works by converting unique tokens into unique integers.
        # Filter 1: Target Column:
        # Filter 2: Hashing Bitsize: 
        # Filter 3: N-grams: 
            # fixed-size array:
            # linked list:
            # hashing:
    # Latent Dirichlet Allocation:
    # Preprocess Text
    # Train Vowpal Wabbit Model: (Deep neural network)
        # 
    # Score Vowpal Wabbit Model:
######################################################################################################
# Computer Vision Modules: (6)
######################################################################################################
# Image Data Transformation:
    # Apply Image Transformation:
    # Convert to Image Directory:
    # Init Image Transformation:
    # Split Image Directory:
# Image Classification:
    # DenseNet:
    # RestNet:
######################################################################################################
# Recommendations Modules: (5)
######################################################################################################
    # Evaluate Recommender:
        # Input 1: Split Data - Test Dataset
        # Input 2: Score Wide and Deep Recommender - Scored Dataset (each user with recommended items, e.g. 2 - 5 items by ranking)
        # Output: 
            # NDCG: (Normalized Discounted Cumulative Gain) (e.g. NDCG = 0.9, is a high-performance value)
            # e.g. search result returns Item 1, Item 2, Item 3, Item  4, Item 5, Item 6
                # However, user has rating Item 1 = 4, Item 2 = 3, Item 3 = 4, Item 4 = 0, Item 5 = 1, Item 6 = 2
                    # In this case, the cumulated gain = 4 + 3 + 4 + 0 + 1 + 2 = 14
                # DCG (Discounted Cumulative Gain) = 4/ log2(1+1) + 3/ log2(2+1) + 4/ log2(3+1) + ... + 2/ log2(6+1) = 8.99
                    # Ideal Ranking = 4/ log2(1+1) + 4/ log2(2+1) + 3/ log2(3+1) + ... + 0/ log2(6+1) = 9.27
                    # Therefore, NDCG = Actual/ Ideal = 8.99/ 9.27 = 0.9697
    # Train SVD Recommender:
        # Train a colllaborative filtering recommendation using SVD algorithm.
    # Train Wide and Deep Recommender:
        # Train a recommender based on Wide & Deep model.
    # Score SVD Recommender:
        # 
    # Score Wide and Deep Recommender:
        # Input 1: (Mandatory) User-Item-Rating triples 
        # Input 2: (Optional) user features
        # Input 3: (Optional) item features
######################################################################################################
# Ahomaly Detection Modules: (2)
######################################################################################################
    # PCA-Based Anomaly Detection:
    # Train Anomaly Detection Model:
######################################################################################################
# Web Service Modules: (2)
######################################################################################################
    # Web Service Input:
    # Web Service Output:
######################################################################################################
# Statistical Functions Modules: (1)
######################################################################################################
    # Summarize Data: 
        # refer to: df.describe(include = 'all')
        # general summary of a dataframe (min, max, mean, mode, number of unique values, missing data count, etc.)
            # Kurtosis, Skewness, Percentile 1, Percentile 95, etc.
            # e.g. how many missing values are there in each column
            # e.g. how many unique values are there in each feature column
            # e.g. what is the mean and standard deviation of each numeric column

######################################################################################################
# Deprecated Modules: (for reference only)
######################################################################################################
# Missing Value Scrubber: (deprecated)
# Export Count Table Module: (deprecated)
# Score Mathbox Recommendation: (deprecated)
    (5 inputs: trained matchbox recommender model, dataset to score, user features, item features, training data)
    (can also be used for "people like you" predictions)
    # Recommender Prediction Kind:
        # Predict Ratings: ()
        # Item Recommendation: 
            (from rated items for model evaluation)
            # Generate a list of items that will appeal to each customer
        # 
    # Recommended Item Selection:
    # Maximum Number of Items to Recommend:
    # Minimum size of recommendation pool for a single user:
    # Whether to return predicted 
######################################################################################################
#
#
######################################################################################################
# Azure ML SDK: 
######################################################################################################
# allow users to manage cloud resources of Azure ML in local Python environment.
# allow users to train models locally with replicable environments.
# available across all Python IDEs and notebooks.
    # Each of the resources is defined as a class in Azure ML SDK:
        # e.g. Workspace(), Environment(), Dataset(), Experiment(), Pipeline(), Run(), Model(), ComputeTarger(), AutoMLConfig(), etc.
            # e.g. we are creating instances of the above classes whe using Azure ML SDK.
    # Install Azure ML SDK --> Anaconda Prompt --> pip install azreml-sdk
# 
#
# Deploy Web Service: (based on score model)
    # Could choose either REST API or excel file
######################################################################################################
# Create New Workspace:
    from azureml.core import Workspace, Datastore
    import os
    os.listdir()
    #
    ws = Workspace.create(name = 'workspace2', subscription_id = '', resource_group = '', create_resource_group = False, location = 'australiaeast')
#
######################################################################################################
# Get datastores from Workspace:
    datastore = Datastore.get(ws, datastore_name = 'your datastore name')
    #
    datastores = ws.datastores
    for name, datastore in datastores:
        print(name, datastore.datastore_type)
#
######################################################################################################
# Create Environment:
    # Option 1: From a specification File:
        from azureml.core import Environment
        env = Environment.from_conda_specification(name = 'test_environment', file_path = './conda.yml')
    # Option 2: From existing conda environment:
        env = Environment.from_existing_conda_environment(name = 'training_environment', conda_environment_name = 'py_env')
    # Option 3: Creating environment by specifying packages:
        env = Environment('New_Environment')
        dependencies = CondaDependencies.create(conda_package = ['scikit-learn', 'pandas', 'numpy'], pip_packages = ['azureml-defaults']) 
######################################################################################################
# Linear Model: (Tensorflow or Pytorch estimator cannot leverage SKLearn model or parameter.)
    from azureml.train.sklearn import SKLearn
    script_params = {'--kernel': 'linear', '--penalty': 1.0 }
    local_folder = ''
    compute_cluster = ''
    sklearn_model1 = SKLearn(source_directory = local_folder, script_paramms = script_params, compute_target = compute_cluster, entry_script = 'sample.py')
######################################################################################################

######################################################################################################
# Exam Structure:
    # 
######################################################################################################
# Exam Questions:
######################################################################################################
# Question: when you create a new ML Worispace, what resources are created alongside with the Workspace?
    # Azure Key Vault, Application Insights, Azure Storage Account, Container Registry.
######################################################################################################
# Question: which VS Code extension enables integrated management of workspace assets?
    # Azure Machine Learning Extension
######################################################################################################
# Question: 
    # Data Engineering and Data Science Environment:
        # Support Python and Scala 
            # Azure Databricks
        # Compose data storage, movement and processing service into automated data pipelines
            # Azure Data Factory
        # The same tool should be used for the orchestration of both data engineering and data science
        # Support workload isolation and interactive workloads
        # enable scaling across a cluster of machines
            # Azure Databricks/ Azure ML Studio
        # You need to create the environment: should build the environment in Azure Databricks and use Azure Data Factory for orchestration.
            # Apache Spark: support Python, R, Scala, SQL
            # High concurrency: Azure Databricks is fully integrated with Azure Data Factory.
                # Azure Container: Azure Container is good for development and testing, not suitable for production workloads.
######################################################################################################
# Question: Introduce Docker for Windows to attendee
    # BIOS-enabled (Basic Input-Output System) virtualization
        # Must make sure that the machine supports Hardware Virtualization and that Virtualization is enabled.
    # Windows 10 64-bit professional
        # To run Docker, your machine must have a 64-bit operating system running Windows 7 or higher.
######################################################################################################
# Question: Select Data Science Environment
    # Azure Cognitive Services: (pre-built deep learning models for video/ language processing.)
        # Azure Cognitive Serviers expand on Microsoft's evolving portfolio of ML APIs ane enable developers to easily add cognitive features:
            # e.g. emotion and video detection, facial, speech, and vision recognition; and speech and language understanding into applications.
            # Azure Cognitive Services can be categorized into 5 main pillars: Vision, Speech, Language, Search and knowledge.
    # Azure Data Lake Analytics:
    # Azure HDInsight with Spark MLLib:
    # Azure ML Studio: 
######################################################################################################
# Question: Move large dataset from Azure ML Studio to Weka environment
    # ARFF
######################################################################################################
# Question: Recommended K value for K-fold Validation: 
    # 5-10
    # 
        from sklearn.model_selection import KFold
        import numpy as np
        #
        X = np.array([1, 2], [3, 4], [5, 6], [7, 8])
        y = np.array([1, 2, 3, 4])
        #
        kf = KFold(n_splits = 2)
        kf.get_n_splits(X)
        print(kf)
        #
        for train_index, test_index in kf.split(X):
            print('Train: ', train_index, 'Test: ', test_index)
            X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]
######################################################################################################
# Question: Difference between DSVM and Azure ML Compute Instance:
    # DSVM (Data Science Virtual Machine)
        # The DSVM is a customized VM image on the Azure Cloud platform built specifically for doing data science.
            # DSVM support Python, R, Julia, SQL, C#, Java, Node.js
            # Operating sytem support Ubuntu and Windows.
            # Allows Scale up option.
            # Without built-in hosted notebooks
            # Pre-installed tools include: Jupyter(Lab), RStudio Server, VSCode, Visual Studio, PyCharm, Juno, Power BI Desktop, SSMS, Office 365, Apache Drill.
    # Azure Machine Learning Compute Instance:
        # Azure ML Compute Instance are fully configured and managed VM image whereas DSVM is an unmanaged VM.
            # Azure ML Compute Instance only support Python and R.
            # Only works on Ubuntu.
            # Allows Scale up option.
            # With built-in hosted Notebooks.
            # Pre-installed tools only include: Jupyter(Lab), RStudio Server
    # DLVM (Deep Learning Virtial Machine)
        # DLVM is a pre-configured environment for deep learning using GPU instance.
######################################################################################################
# Question: Classification task, dataset is inbalanced, how to improve classification accuracy?
    # SMOTE (Synthetic Minority Oversampling Technique)
        # 200 SMOTE percentage would increase 1% minority class to around 3% (2/101)
#
######################################################################################################
# Question: You can move data to and from Azure Blob using different technologies include:
    # Azure Storage Explorer
    # AzCopy
    # Python/ Pyspark
    # SSIS 
    # Bulk Copy Program (BCP)
######################################################################################################
# Question: 
    # Weka: weka is used for visual data mining and machine learning software in Java.
    # Rattle: Rattle is the R analytical tool that gets you started with data analytics and machine learning.
######################################################################################################
# Question: if you are planning to use a DSVM with the open source deep learning framework Caffe2 and PyTorch: 
    # Use DSVM for Linux (Ubuntu)
######################################################################################################
# Question: compute target to deploy the workspace
    # Azure Container Service (allow deployment) (retired on Jan 31, 2020, replaced by AKS)
    # Azure Kubernates Service
        # (Azure Data Lake Analytics, Databricks, HDInsights do not allow deployment option.) 

######################################################################################################
# Question: what is a workspace in Azure ML?
    # 

######################################################################################################
# Question: how to deal with new data input that does not exist in the training dataset.
    # 

######################################################################################################
# Question: script to retrieve the output file names:
    # 
######################################################################################################
# Question: deploy the model to a context that allows for real-time GPU-Based inferencing:
    # Should use compute type: Azure Kubernates Service
        # For web-service deployment, GPU inferencing is only supported on Azure Kubernates Service.
        # For inferencing using a Machine Learning pipeline, GPUs are only supported on Azure Machine Learning Compute.
######################################################################################################
# Question: Batch inferencing, need to monitor the progress of the pipeline execution:
    # Batch inferencing service require a scoring script to load the model and use it to predict new values. It must include 2 functions: init() and run(mini_batch)
        # Typically, you use the init() function to load the model from the model registry, and use run() to generate predictions.
            import os
            import numpy as np
            from azureml.core import Model
            import joblib
            #
            def init():
                global model
                model_path = Model.get_model_path('model_name')
                model = joblib.load(model_path)
            def run(mini_batch):
                result_list = []
                for file in mini_batch:
                    data = np.genformtxt(file, delimiter = ',')
                    prediction = model.predict(data.reshape(1, -1))
                    result_list.append('{}: {}'.format(os.path.basename(file), prediction[0]))
                return result_list
######################################################################################################
# Question: how to ensure the correct version of Pytorch can be identified for the inferencing environment when the model is deployed?
    # 
######################################################################################################
# Question: Need to normalize values to produce an output column into bins to predict target column:
    # Entropy MDL Bining Mode:
######################################################################################################
# Question: When needed to tune hyperparameters, and we need to iterate all possible combinations while minimizing computing resources required.
    # Use Random Grid 

######################################################################################################
# Question: Difference between Azure ML Service and Azure ML Studio?
    # Azure ML Studio is just the UI, which is not the core service
######################################################################################################
# Question: need to register the new version of a model while keeping the current version of the model in the registry, how could we do that?
    # Register the model with the same name as the existing model.
######################################################################################################
# Question: you need to normalize values to produce a feature column grouped into Bins:
    # apply an Entropy Mimimum Description Length (MDL) binning mode 
######################################################################################################
# Question: how do you record unique label values as run metrics?
    # Script:
    import pandas as pd
    run = Run.get_context()
    data = pd.read_csv('file1.csv')
    labels = data['label'].unique()
    for label in labels:
        run.log('Label Value', label)
######################################################################################################
# Question: which metrics should we use to evaluate model results for imbalance. (binary classification model)
    # AUC Curve
        # The higher the AUC, the better performance of the model at distinguishing between the positive and negative classes.
            # When AUC = 0.5, the 2-class classifier is not able to distinguish positive and negative classes.
        # Y-axis: True-Positive Rate (TPR = TP/ (TP + FN) = Recall = Sensitivity = ability to recognise TP from all True)
        # X-axis: False-Positive Rate (FPR = FP/ (FP + TN) = fall-out = Inability to recognise TN from all False)
            # By choosing different threshold, e.g. [0.0 1, 0.02, 0.03, ... 0.99], and plot the TPR-FPR on the X, Y axis.
            # AUROC = 
######################################################################################################
# Question: Which mode should you use to split a "time-series" dataset in Azure ML Studio into training and test sets？
    # Relative Expression Split
######################################################################################################
# Question: how to reduce over-fitting for CNN Model?
    # Solution 1：Add L1/L2 regularization
    # Solution 2: Use training data augmentation
    # Solution 3: Increase Training Data Size
    # Solution 4: Add Dropout Layers
######################################################################################################
# Question: When a model is over-fitted, how is the loss in both the training and test set changing?
    # The training loss would be decreasing while the validation loss is actually increasing.
######################################################################################################
# Question: The binary classification model training set is imbalanced, how to we resolve the data imbalance?
    # Solution 1: Penalize the Classification
    # Solution 2: Resample the dataset using under-sampling and over-sampling
    # Solution 3: Generate synthetic samples in the minority class
######################################################################################################
# Question: Feature Scaling --> original and scaled data:
    # Scenario 1: Standard Scaler (mean = 0, std)
        # Z = (x - miu)/ std()
    # Scenario 2: Min-Max Scaler ([0, 1])
    # Scenario 3: Normalizer 
######################################################################################################
# Question: You are producing a multiple linear regression model, and several independent variables are highly correlated.
    # You need to select appropriate methods for feature engineering:
        # Step 1: Use the Filter-based Feature Selection Module
        # Step 2: Build a counting transformation
        # Step 3: Test the hypothesis using t-test
######################################################################################################
# Question: Azure CLI - automate the creation of Azure ML workspace:
    # Step 1: az extension add -n azure-cli-ml # register the ML extension to run Azure ML commands
    # Step 2: az group create --name<resource-group-name> --location<location> # create resource group where Azure ML Workspace could reside in
    # Step 3: az ml workspace create -w<workspace-name> -g<resource-group-name> # create workspace in the resource group
######################################################################################################
# Question: Azure CLI - update the storage keys in the workspace (after workspace is compromised and regenerated storage account keys)
    # az ml workspace sync-keys
######################################################################################################
# Question: You use Azure ML to deploy ML models, and want to be notified when a model deployment fails:
    # You should: Stream Azure ML Logs to Azure Monitor.
        # Azure Event Hub: focused on data analysis to discover actionable insights/ BI.
            # You cannot create alerts in Azure Event Hub.
        # Query AmlComputeJobEvents using the ExecutionState property:
            # Does provide the job execution state, whereas do not provide alerts.
        # Query AmlComputeJobEvents using the ProvisionState property:
            # Record the state of the job submission, whereas not the state of the job itself.
######################################################################################################
# Question: You are using Azure ML to deploy a ML model to a AKS cluster, during the test, you received a large number of HTTP 503 errors.
    # And you need to reduce the instances of HTTP 503 errors.
        # HTTP 503 Service Unavailable error indicates that the service is operational, whereas it's unable to respond to request.
            # This often indicates that the server is overloaded and does not have the resources to process the request.
                # Option 1: Change the utilization level at which containers autoscale up
                # Option 2: Change the minimum number of replicas.
                    # Minimum number pf replicas = minimum number of nodes that should be online at any time, default value = 1.
                # Option 3: Modify the autoscale_max_replicas parameter 
                    # Maximum number of replicas = maximum number of nodes that can be operating, default vlaue = 10.
######################################################################################################
# Question: Use local computer (Anaconda Environment) as a development environment to work with Azure ML, and you want to leverage Automated ML to
    # tune hyperparameters for your training pipelines.
        conda create -n devenv python = 3.7.7
        conda activate devenv
        conda install notebook ipykernel
        ipython kernel install --user --name devenv --display-name "Python(devenv)"
        pip install azureml-sdk[notebooks, automl]
######################################################################################################
# Question: logging method to generate required logging information:
    #
        # Option 1: run.log()
            # can be used to log scalar string or numeric values
        # Option 2: run.log_list()
            # when needing to log list of values, arrays, dictionaries, vectors or any other non-scalar 
        # Option 3: run.log_image()
            # to log transformed PNG images (or matplotlib plots) to the run record
######################################################################################################
# Question: how to create an Azure ML workspace using Azure Cloud Shell?
    #
    az ml workspace create -w AML-WorkspaceName -g AML-LearningResourceGroupName
######################################################################################################
# Question: You need to enable logging the application state during the training process:
    #  the experiment.submit() method can be configured with the show_output parameter to enable local logging during the training process.
        from azureml.core import Experiment
        experiment = Experiment(workspace = ws, experiment_name = experiment_name)
        run = experiment.submit(config = run_config_object, show_object = True)
######################################################################################################
# Question: different logging stage:
    # Stage 1: during a compute target creation: (compute.wait_for_completion(show_output = True))
        from azureml.core.compute import ComputeTarget
        compute_target = ComputTarget.attach(workspace = ws, name = 'example_name', attach_configuration = config)
        compute.wait_for_completion(show_output = True)
    # Stage 2: during the training process: 
        from azureml.core import Experiment
        experiment = Experiment(ws, experiment_name)
        run = experiment.submit(config = run_config_object, show_output = True)
    # Stage 3: logging for run-related data within the experiment: (.start_logging())
        #
    # Stage 4: logs for previously deployed web service: (.get_logs())
######################################################################################################
# Question: how to ensure that the output files are uploaded in real time?
    experiment_output = os.path.join(os.curdir, 'logs')
    # Files written to ./logs are uploaded in real time.
        # files written to the output folder persist across experiments, but they are not uploaded in real time.
######################################################################################################
# Question: how to ensure that a use can submit a training run? (you created a custom role definition file)
    # Add workspace/ environment to the actions elements
######################################################################################################
# Question: how to bookmark the state of your data prior to retraining?
    # use the register model from the dataset class to create a new dataset version.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

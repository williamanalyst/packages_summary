# import pyspark.sql.functions to use SQL commands to manipulate datasets (e.g. parquet files)
import pyspark.sql.functions as F # 
#
dynamicparts=spark.read.parquet('/mnt/margin_management/daily/DynamicPrices_'+today+'.parquet') # read the .parquet file from Azure Blob
#
display(dynamicparts.filter(F.col('IPD_Dynamically_Priced')=='Y')) # filter the data table with certain criteria
df2 = df.filter(F.col('date_col') == '2020-12-12')


############################################################################################
# Check general features of a pysparl.sql.dataframe
# 
df = spark.read.parquet('/folder/filename.parquet') # read .parquet files as dataframes in pyspark
print('Shape of the pyspark dataframe = ({}, {})'.format(p2s_daily.count(), len(p2s_daily.columns))) # returns the shape of the pyspark dataframe
#
df.schema
df.printSchema() # print out the datatypes of a pyspark dataframe
#
# Unique values
df.select('col1').distinct().count()



############################################################################################
# groupBy function
df_grouped = df.groupBy(['col1', 'col2']).count().orderBy(['col1', 'col2'], ascending = [1, 1])
df_grouped.show()




############################################################################################
# Date manipulation
# from pyspark.sql.functions import col, unix_timestamp, to_date
#
df = df.withColumn('new_col', to_date(unix_timestamp(col('date_string'), 'yyyy-MM-dd' HH:MM),cast('timestamp'))) # must nominate the format of the original date_string


############################################################################################
# combine/ concat 2 dataframes
df = df1.union(df2)







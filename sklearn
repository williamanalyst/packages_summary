# split train and test dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y) #

# K-NN (K-nearest neighbors, to predict, choose the closest K points of the item to be predicted, 
    # check to which group do they belong to, and classify it to the group with highest frequency.)
        # large K = smoother decision boundary = less complex model
        # small K = more complex model = risk of over-fitting
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborClassifier(n_neighbors = 6)
knn.fit(X_train, y_train)
#
prediction = knn.predict(X_test)
knn.score(X_train, y_train) # accuracy on the training dataset
knn.score(X_test, y_test) # accuracy on the test dataset

# linear regression (ordinary least squares = minimize the sum of residual-squared )
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
#
linearreg = LinearRegression()
linearreg.fit(X_train, y_train) #
#
y_pred = linearreg.predict(X_test)
linearreg.score(X_test, y_test)
linearreg.score(X_test, y_test)
rmse = np.sqrt(mean_squared_error(y_pred, y_test)) # calculate the error/loss function value 

# k-means clustering
# 
from sklearn.preprocessing import StandardScaler

#
ss = StandardScaler()
df_scaled = ss.fit_transform(df_selected.astype(float)) # standardise the input variables

#
kmeans = KMeans(n_clusters = 3, random_state = 42).fit(df_scaled) # use k-means method to classify the items/observations 
labels = kmeans.labels_


# 

# Pandas Package Common Usage:
############################################################################################
# Basic Settings of Pandas:
############################################################################################
import pandas as pd
pd.set_option('display.max_columns', 100, 'display.max_rows', 200)
from IPython.display import display # for better formatted tables to be displayed
# 
df.describe(include = 'all', 
percentiles = [0.01, 0.05, 0.1, 0.9, 0.95, 0.99, 0.995]) # provide basic statistics for each column
df.shape # 
df.dtypes # 
df['col1'].value_counts() # get the number of occurances of a variable 
df.head().index # get the index of the head rows
#
# Select only numeric columns:
df_numeric = df.select_dtypes(include = ['int', 'float'])
#
df_filled = df.fillna(subset = ['col1'], value = 0)
#
############################################################################################
# Obtain or Remove Duplicated value:
############################################################################################
df.duplicated(subset = ['col1', 'col2'], keep = False) # evaluate whether a row is duplicated, return True/ False as an array
df.duplicated(subset = ['col1', 'col2'], keep = last) # by using 'last', the last occurrence of each set of duplicated value is set on False, and all others on True

#
############################################################################################
# Get the numeric columns
df_numeric_columns_names = df.select_dtypes(include = 'number').columns
df_numeric_columns = df.select_dtypes(include = 'number')
#
df_object_columns_names = df.select_dtypes(include = 'object').columns
df_numeric_columns = df.select_dtypes(include = 'object')
#
df_select_by_type = df_select_dtypes(include = ['int64', 'float64', 'object']) # 
df_select_by_type = df_select_dtypes(exclude = ['int64', 'float64', 'object']) # 
############################################################################################
# Change Data Type
############################################################################################
df['col1'] = df['col1'].astype('category') # could also use 'int', 'float', 'str', etc.
df['col1'] = pd.to_datetime(df['col1']) # change into datetime
#
############################################################################################
# Change object type columns into Categorical Variables
    columns = df.select_dtyped(include = 'object').columns
    df[columns] = df[columns].astype('category')
    #
    df['col1'] = df['col1'].cat.codes
    for column in columns: # 
        df[column] = df[column].cat.codes
############################################################################################
# One-Hot Encoding/ Dummy Variables Creations:
    #
    df_dummy = pd.get_dummies(df, columns = ['country'], prefix = 'C')
    df_dummy = pd.get_dummies(df, prefix = '', prefix_sep = '', columns = ['col1', 'col2'], 
                                drop_first = True) # get k-1 columns of dummy variables
    # 
############################################################################################
# Group by and Partition By and Ranking:
############################################################################################
df['Rank'] = df.groupby(['col1'])['col2'].rank(method = 'min', ascending = True) # create a rank for column 2 in each group of column 1 in the table

############################################################################################
# loading data
############################################################################################
# Read .csv/ .tsv files
df = pd.read_csv('sample_file.csv', dtype = {'col1': 'str'}, parse_dates = ['col2', 'col3']) # comma separated file
df = pd.read_csv('sample_file2.txt', sep = '\t', header = None) # tab separated file
df = pd.read_csv('https://www.test.com/datafile.data', header = None, names = ['co11', 'col2', 'col3']) # if the table does not have a header
#
# Read .xlsx files
df = pd.read_excel('sample_file2.xlsx', sheet_name = 'sheet2', na_value = 'Missing_Value', skiprows = 2,
    dtype = {'column_name1': 'str', 'column_name2': 'float', 'column_name3': 'int'})
#
# rename column names
df.rename(columns = {'col1': 'renamed1', 'col2': 'renamed2'}, inplace = True)
#
df.isnull() 
df_count_null = df.isnull().sum()
############################################################################################
# filter dataframe with .loc
df_t11 = df.loc[(df['column1'] >= 1) & (df['column2'] == 'text') | (df['column3'] < 5)]
#
df_selected = df.loc[(df['text_column'].str.contains('certain string'))] # returns all the rows with a certain string contained in a given column

# groupby by and aggregate by function
df.groupby(['column1', 'column2'], as_index = False).agg({'column11': 'count', 'column12': 'sum', 'column13': 'nunique', 
'column14': 'mean', 'column15': 'min', 'column16': 'max'}).sort_values(by = ['column1'], ascending = [False])
#
# Get the max value in each column
max_values = df.max(axis = 1, skimna = False)
print(max_values)

# Get the max value in each column in each group

############################################################################################
# Process Outliers:
    # clip
    df[[col1]].clip(0, 100) # any value below 1 will be replaced by 1, and value above 100 will be replced by 100
    for column in df.select_dtypes(include = 'number').columns:
        lower = df[column].percentile(0.01)
        upper = df[column].percentile(0.99)
        df[column_clipped] = df[column].clip(lower, upper)
        print(column, lower, upper)
############################################################################################
# .pct_change()

############################################################################################
# append data to existing dataframe
# Append Rows:
df = df.append({'col1': value1, 'col2': 'str2'}, ignore_index = True)
# Append Columns:
df_concat = pd.concat([df1, df2], ignore_index = True) # 
#
############################################################################################
# Drop Columns:
############################################################################################
# df = df.iloc[:, :-1]
# df = df.drop(['col1', 'col2', 'col3'], axis = 1)
#
df_clean1 = df.dropna()
df_clean2 = df.dropna(axis = 1)
df_clean3 = df.
#
############################################################################################
# .shift() to apply either lag or ahead
df['col1_lag1'] = df.groupby(['group_col'])['col1'].shift(1)
df['col1_lead1'] = df.groupby(['group_col'])['col1'].shift(-1)

############################################################################################
# pivot_table
############################################################################################
df_pivot = pd.pivot_table(df, index = ['index_row'], columns = ['index_column'], 
values = ['col_agg1', 'col_agg2'], aggfunc = {'col_agg1': 'sum', 'col_agg2': 'mean'})
# Melt Pivot Table
df = pd.DataFrame({'': })
pd.melt(df, id_vars = ['categoty'], value_vars = ['B', 'C'])
############################################################################################
#
############################################################################################
# Calculation based on columns
df['cal_col'] = df.apply(lambda x: x['col1'] if x['col1'] > x['col2'] else x['col2'], axis = 1)


# subtract 1 dataframe from another dataframe
df1 = pd.DataFrame({'col1': [10, 100, 1000], 'col2': [1, 2, 3]})
df2 = pd.DataFrame({'col1': [10, 100, 1000], 'col2': [100, 10, 1]})
df3 = df1.subtract(df2)
display(df3)


# change integer to string with zero at the beginning (different length)
df1['filled'] = df1['col1'].apply(lambda x: '{0:0>8}'.format(x))
print('Print the value in a float {:.2f} format: '.format(x)) # print the value as float, with 2 decimal points
print('Print the value in a percentage {0:.2%} format: '.format(x)) # print the vlaue as percentage, with 2 decimal points
print('Print the value in a thousand {L,}'.format(x))

# calculate the number of days between 2 timedelta objects
df['year'] = df['col1'].dt.year
df['date'] = df['col1'].dt.date # this create a string column that keeps only the %Y-%m-%d section of the date column
df['month'] = df['col2'].dt.month # 

#
df['weekday'] = df['col3'].dt.dayofweek # obtain the weekday of the date column
df['days_in_between'] = (df['end_date'] - df['start_date'])/ pd.Timedelta(1, unit = 'd')
df['months_in_between'] = df['date_column'].dt.to_period('M') - certain_date.to_period('M') # returns the number of months from the starting month/ date
#
############################################################################################
# create a date list (list of dates)
############################################################################################
date_list = pd.date_range(start = '2020-01-01', end = '2020-12-31')
date_list2 = pd.date_range(start = '2020-01-01', end = '2020-12-31', freq = 'MS') # this creates a Pandas DatetimeIndex object
#


# cut function to divide a dataframe into several groups
df['col1'] = pd.cut(df['col_to_be_divided'], [0, 2, 4, 6, 8]) # based on the list, the array is divided into (a, b], (b, c], etc.

# 
df['sum_all_columns'] = df.apply(sum, axis = 1) # 
df['max_column'] = np.maximum(df['col1'] * 2, df['col2'] + df['col3']) # array conversion


# crosstab function (similar to pivot_table function) 
pd.crosstab(df['col1'], df['col2'], values = df['col2'], aggfunc = 'mean').round()
#
############################################################################################
# Join tables:
############################################################################################
#
df_combined = pd.merge(left = df1, right = df2, left_on = ['col1'], right_on = ['col2'], how = 'left')
df_combined = pd.merge(left = df1, right = df2, left_on = ['col1'], right_on = ['col2'], how = 'right')
df_combined = pd.merge(left = df1, right = df2, left_on = ['col1'], right_on = ['col2'], how = 'inner')
df_combined = pd.merge(left = df1, right = df2, left_on = ['col1'], right_on = ['col2'], how = 'outer')


############################################################################################
# Save files as output:
############################################################################################
# Save file as .csv
df.to_csv('filename.csv', header = True, 
    index = False) # not saving index column
# Save file as .tsv
df.to_csv('filename.txt', header = True, index = False, sep = '\t') # save as .tsv file 
df.to_excel('filename.xlsx', sheet_name = 'Sheetname1', index = False) # save as .xlsx file



############################################################################################
# Integration with Plotly
import pandas as pd
pd.options.plotting.backend = 'plotly'
#
df = pd.DataFrame(dict(a = [1, 2, 3], b = [3, 2, 1]))
fig = df.plot()
fig.show()
#
############################################################################################
# Read extremely large files:
records = []
for chunk in pd.read_csv('bigfile.csv', chunksize = 1000):
result.append(sum(chunk['column_to_be_sumed']))
result = sum(records)
#
############################################################################################
# rolling mean/ average (window function)
df['7_day_rolling'] = df['column1'].rolling(window = 7, center = False).mean()
df['7_day_rolling'] = df['column1'].rolling(window = 7, center = False).sum()
#
df['cumulative_sum'] = df['col1'].cumsum() # 
#
# Calculate the difference with the next (time) period
df['next_minus_current'] = df['col'].diff(1) # equal to t(n) - t(n-1)
df['current_minus_next'] = df['col'].diff(-1)
#

 
 

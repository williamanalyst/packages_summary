# import pyspark.sql.functions to use SQL commands to manipulate datasets (e.g. parquet files)
import pyspark.sql.functions as F # 
#
%md # change the whole cell into markdown
############################################################################################
# Load data from mounted folder:
dynamicparts=spark.read.parquet('/mnt/margin_management/daily/DynamicPrices_'+today+'.parquet') # read the .parquet file from Azure Blob
df = spark.read.parquet('/folder/filename.parquet') # read .parquet files as dataframes in pyspark
df1 = spark.read.format('csv').options(header = 'True', delimiters = ',', inferSchema = 'False').load('/mnt/folder/file.csv') # 
df2 = spark.read.format('com.crealytics.spark.excel').option('header', True).load('.xlsx') # load 
#
display(dynamicparts.filter(F.col('IPD_Dynamically_Priced')=='Y')) # filter the data table with certain criteria
df2 = df.filter(F.col('id').isin(['11111', '11112', '11113', '11114']))
df2 = df.filter(F.col('date_col') == '2020-12-12')
df3 = df.filter(F.col['col2'].isNull())
df3 = df.filter(F.col['col2'].isNotNull())
#
############################################################################################
# Check general features of a pyspark.sql.dataframe
# 
############################################################################################
# Print out the Shape of a Dataframe:
print('Shape of the pyspark dataframe = ({}, {})'.format(p2s_daily.count(), len(p2s_daily.columns))) # returns the shape of the pyspark dataframe
df.columns # would return the column names of the dataframe, similar to Pandas package
#
############################################################################################
df.schema # shown as a list
df.printSchema() # print out the datatypes of a pyspark dataframe
#
############################################################################################
# Unique values
df.distinct().count() # get the number of unique rows
df.select('col1').distinct().count() # get the number of unique value in a column

############################################################################################
# groupBy function
df_grouped = df.groupBy(['col1', 'col2']).count().orderBy(['col1', 'col2'], ascending = [1, 1])
df_grouped.show()
    # 



############################################################################################
# Date manipulation
# from pyspark.sql.functions import col, unix_timestamp, to_date
#
# Change the data type when creating or updating columns
df = df.withColumn('new_col', to_date(unix_timestamp(col('date_string'), 'yyyy-MM-dd' HH:MM),cast('timestamp'))) # must nominate the format of the original date_string
df.withColumn('col2', df['col1'].cast('int'))\
.withColumn('col3', df.col1.cast('integer'))\
.withColumn('col4', df.col1.cast('float')) # 'double', etc.

############################################################################################
# combine/ concat 2 dataframes
df = df1.union(df2)
#

############################################################################################
# Join 2 tables:
df_joined = df1.join(df2, F.col('col_name1') == F.col('col_name2'), how = 'left') # could also be right join, inner join, outer join, etc.
df_joined = df1.join(df2, ['col1', 'col2'], how = 'leftanti') # 

############################################################################################
# Calculate new column based on existing columns:
#
df.withColumn('col1', F.lit(None).astype('float'))
df.withColumn('col2', F.when(df['col3'].isNull(), 0)
                       .when(df['col4'].isNotNull(), 1)
                       .otherwise() )
#
df.withColumn('PPV_Cost', F.col('PPV_Cost').cast('float'))
#
############################################################################################
# Fill in missing values:
df.fillna(99999, subset = ['col1'])


############################################################################################
# Drop Columns:
df = df.drop('col1')



############################################################################################
# Get the aggregation for certain column:
max_line_number = df_spark.agg({'col1': 'max'}).show()



############################################################################################
# import specific datatypes
from pyspark.sql.types import IntegerType

############################################################################################
# udf manipulating a certain column
from pyspark.sql.types import IntegerType
add_n = udf(lambda x, y: x + y, IntegerType())
df_test = df_test.withColumn('New_Line_Number', add_n(F.lit(1000), df_test.QTY_BREAK_1.cast(IntegerType())))
display(df_test)


############################################################################################
# Add leading zeros in the beginning:
df = df.withColumn('col1', F.lpad(F.col('column_name'), 4, '0') # add leading zeros and make the columns to be a string with certain length
#
############################################################################################
# Retrieve the values of a column:
############################################################################################
list1 = list(df.select('col1').collect())
list_keep_part_only = []
for i in list1:
    list_keep_part_only.append(i[0])

############################################################################################
# Convert Pandas dataframe into Spark DataFrame
df_spark = spark.createDataFrame(df_pandas)

# Save the file as .xlsx file
df_spark.write.format('com.crealytics.spark.excel').option('header', 'true').save('/mnt/mounted_folder_name/filename.xlsx', mode = 'overwrite')

############################################################################################
# Drop duplicated rows
df_spark = df_spark.dropDuplicates(['col1', 'col2'])

############################################################################################
# Get the current user who is running the script/ Azure Databricks Notebook:
############################################################################################
# returns: 
dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')




############################################################################################
# Convert to Pandas DataFrame:
############################################################################################
df_pandas = df_pyspark.toPandas()
























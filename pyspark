# import pyspark.sql.functions to use SQL commands to manipulate datasets (e.g. parquet files)
import pyspark.sql.functions as F # 
#
############################################################################################
# Load data from mounted folder:
dynamicparts=spark.read.parquet('/mnt/margin_management/daily/DynamicPrices_'+today+'.parquet') # read the .parquet file from Azure Blob
df = spark.read.parquet('/folder/filename.parquet') # read .parquet files as dataframes in pyspark
#
display(dynamicparts.filter(F.col('IPD_Dynamically_Priced')=='Y')) # filter the data table with certain criteria
df2 = df.filter(F.col('date_col') == '2020-12-12')
df3 = df.filter(F.col['col2'].isNull())
df3 = df.filter(F.col['col2'].isNotNull())
#
############################################################################################
# Check general features of a pyspark.sql.dataframe
# 
############################################################################################
# Print out the Shape of a Dataframe:
print('Shape of the pyspark dataframe = ({}, {})'.format(p2s_daily.count(), len(p2s_daily.columns))) # returns the shape of the pyspark dataframe
#
############################################################################################
df.schema # shown as a list
df.printSchema() # print out the datatypes of a pyspark dataframe
#
############################################################################################
# Unique values
df.select('col1').distinct().count()

############################################################################################
# groupBy function
df_grouped = df.groupBy(['col1', 'col2']).count().orderBy(['col1', 'col2'], ascending = [1, 1])
df_grouped.show()
    # 



############################################################################################
# Date manipulation
# from pyspark.sql.functions import col, unix_timestamp, to_date
#
df = df.withColumn('new_col', to_date(unix_timestamp(col('date_string'), 'yyyy-MM-dd' HH:MM),cast('timestamp'))) # must nominate the format of the original date_string


############################################################################################
# combine/ concat 2 dataframes
df = df1.union(df2)
#

############################################################################################
# Join 2 tables:
df_joined = df1.join(df2, F.col('col_name1') == F.col('col_name2'), how = 'left') # could also be right join, inner join, outer join, etc.
df_joined = df1.join(df2, ['col1', 'col2'], how = 'leftanti') # 

############################################################################################
# Calculate new column based on existing columns:
#
df.withColumn('col1', F.lit(None).astype('float'))
df.withColumn('col2', F.when(df['col3'].isNull(), 0)
                       .when(df['col4'].isNotNull(), 1)
                       .otherwise() )

############################################################################################
# Fill in missing values:
df.fillna(99999, subset = ['col1'])


############################################################################################
# Drop Columns:
df = df.drop('col1')



############################################################################################
# Get the aggregation for certain column:
max_line_number = df_spark.agg({'col1': 'max'}).show()



############################################################################################
# import specific datatypes
from pyspark.sql.types import IntegerType

############################################################################################
# udf manipulating a certain column
from pyspark.sql.types import IntegerType
add_n = udf(lambda x, y: x + y, IntegerType())
df_test = df_test.withColumn('New_Line_Number', add_n(F.lit(1000), df_test.QTY_BREAK_1.cast(IntegerType())))
display(df_test)

















